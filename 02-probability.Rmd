# Probability

## Chapter Preview {-}
Simply put, probability is the study of randomness. In this chapter, we will define probability, learn rules of probability, and apply these rules to sports data.

## Definitions

::: {.definition}
An ***experiment*** is any activity or process whose outcome is subject to uncertainty.
:::


::: {.definition}
The ***sample space*** of an experiment, denoted by $\Omega$ or $\mathcal{S}$, is the set of all possible outcomes of that experiment.
:::


::: {.definition}
An ***event*** is any collection (subset) of outcomes contained in the sample space, $\Omega$.
:::

::: {.example}
$$ $$
:::
\
\
\
\
\

::: {.example}
$$ $$
:::
\
\
\
\
\

## Set Theory

For the following examples, suppose that we are interested in the batting outcomes of a plate appearance in softball.

Let $A$ be the event that the batter gets walked, let $B$ be the event that the batter gets a hit, let $C$ be the event that the batter strikes out, and let $D$ be the event that the batter makes it to first base at the end of their at bat.

We will define a handful of set operations to help us when we begin calculating the probability of different events occurring. 

::: {.definition}
The ***compliment*** of an event $A$, denoted by $A^c$ or $A'$, is the set of all outcomes in $\Omega$ that are not contained in $A$.
:::

::: {.example}
Draw a Venn diagram illustrating $A^c$ and describe the event.
:::

\
\
\
\
\

::: {.definition}
The ***union*** of two events $A$ and $B$, denoted by $A \cup B$ and read "$A$ or $B$", is the event consisting of all outcomes that are either in $A$ or $B$ or in both.
:::

::: {.example}
Draw a Venn diagram illustrating $A \cup D$ and describe the event.
:::

\
\
\
\
\

::: {.definition}
The ***intersection*** of two events $A$ and $B$, denoted by $A \cap B$ and read "$A$ and $B$", is the event consisting of all outcomes that are in both $A$ and $B$.
:::

::: {.example}
Draw a Venn diagram illustrating $A \cap D$ and describe the event.
:::

\
\
\
\
\

::: {.definition}
The ***difference*** of two events $A$ and $B$, denoted by $A \mathbin{/} B$ and read "difference of $A$ and $B$", is
the event consisting of all outcomes that are in $A$ but not in $B$.
:::

::: {.example}
Draw a Venn diagram illustrating $D \mathbin{/} A$ and describe the event.
:::

\
\
\
\
\



::: {.definition}
Two events $A$ and $B$ are said to be ***disjoint*** (or ***mutually exclusive***) if $A \cap B = \emptyset$
:::

::: {.example}
Are the events $A$ and $B$ disjoint? How about $A$ and $D$?
:::

\
\
\
\
\




## Probability Axioms and Properties

There are some basic assumptions of "axioms" which are the foundation of the theory of probability. Andrey Kolmogorov first described these axioms in 1933.

### Axioms of Probability
1. $P(A) \geq 0$, for any event $A$ \
2. $P(\Omega) = 1$ \
3. If $A_1, A_2, A_3, \ldots$ is a collection of disjoint events, then: \
$P(\cup_{i=1}^{\infty} A_i) = P(A_1 \cup A_2 \cup \ldots ) = \sum_{i=1}^{\infty} P(A_i)$

Note that all probabilities are between 0 and 1, that is, for any event $A$, $0 \leq P(A) \leq 1$.

We can convert to percentages by multiplying probabilities by 100, however, this is a set that is only done after all calculations have been completed.

### Properties of Probability

- $P(\emptyset) = 0$ \

- $P(A^c) = 1 - P(A)$ 

- $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

- $P(A \cup B \cup C) = \\ P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$

- $P([A \cup B]^c) = P(A^c \cap B^c)$

- $P([A \cap B]^c) = P(A^c \cup B^c)$

## Laws of Probability

::: {.definition}
Let $A$ and $B$ be two events such that $P(B)>0$. Then the ***conditional probability*** of $A$ given $B$, written $P(A|B)$, is given by:
$P(A|B) = \frac{P(A \cap B)}{P(B)}$
:::

:::{ .example}
In 2001, Barry Bonds broke the single season home run record with 73 home runs. In this season, he had 664 plate appearances, 156 hits, 177 walks and 9 hit by pitches. Given that Bonds reached base (via hit, walk, or HBP), what was the probability that he got a hit?
:::

\
\
\

:::{.theorem name="Multiplication Rule"}
For any two events $A$ and $B$, $P(A \cap B) = P(B|A) \cdot P(A)$.
:::

:::{.definition}
Events $A_1, A_2, \ldots, A_n$ are said to form a ***partition*** of a sample space $\Omega$ if both: \
(i) $A_i \cap A_j = \emptyset$ ($i \neq j$) \
(ii) $\cup_{i=1}^n A_i = \Omega$ \
:::


:::{.theorem name="Law of Total Probability"}
Suppose events $A_1, A_2, \ldots, A_n$ form a partition of $\Omega$, then:
$P(B) = P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + \ldots P(B|A_n)P(A_n)$
:::



:::{.theorem name="Bayes Theorem: simple version"}
Suppose events $B$ and $C$ form a partition of $\Omega$, then:
$P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|C)P(C)}$
:::

:::{.theorem name="Bayes Theorem"}
Suppose events $B_1, B_2, \ldots, B_n$ form a partition of $\Omega$, then:
$P(B_k|A) = \frac{P(B_k \cap A)}{P(A)} = \frac{P(A|B_k)P(B_k)}{P(A|B_1)P(B_1)+P(A|B_2)P(B_2) + \ldots + P(A|B_n)P(B_n)}$
:::

## Combinatorics

Combinatorics is the mathematical study of counting, particularly with respect to permutations and combinations.

:::{.definition}
The ***factorial function ($n!$)*** is defined for all positive integers by: $n! = n \cdot (n-1) \cdot \ldots 2 \cdot 1$
:::

Note that $0! \equiv 1$ and $1! \equiv 1$.

:::{.definition}
An ordered subset is called a ***permutation***. The number of permutations of size $k$ that can be formed from the $n$ elements in a set is given by: $P_{n,k} = \frac{n!}{(n-k)!}$
:::

:::{.definition}
An unordered subset is called a ***combination***. The number of combinations of size $k$ that can be formed from the $n$ elements in a set is given by: $C_{n,k} = {n \choose k} = \frac{n!}{k! \cdot (n-k)!}$
:::

:::{.theorem name="Product Rule for Ordered Pairs"}
If the first element of an ordered pair can be selected in $n_1$ ways and for each of the these $n_1$ ways the second element of the pair can be selected in $n_2$ ways, then the number of pairs is $n_1 \cdot n_2$.
:::

:::{.theorem name="Generalized Product Rule"}
Suppose a set consists of $k$ elements (k-tuples) and that there are $n_1$ possible choices for the first element, $n_2$ possible choices for the second element, ... , and $n_k$ possible choices for the $k^\text{th}$ element, then there are $n_1 \cdot n_2 \cdot \ldots \cdot n_k$ possible k-tuples.
:::

## Odds and Gambling


## Random Variables

:::{.definition}
Let $\Omega$ be the sample space of an experiment. A ***random variable*** is a rule that associates a number with each outcome in $\Omega$. In other words, a random variable is a function whose domain is $\Omega$ and whose range is the set of real numbers.
:::

Random variables are be broken down into subcategories:\
1. ***Discrete random variables*** - random variables which have a sample space that is finite or countably infinite.\
2. ***Continuous random variables*** - random variables which have a sample space that is uncountably infinite (such as an interval of real numbers)

***Discrete*** and ***Continuous*** random variables use similar yet slightly different mathematical tools. Discrete random variables involve working with "sums" and continuous random variables involve working with "integrals".

::: {.example}
$$ $$
:::

::: {.example}
$$ $$
:::

:::{.definition}
A ***probability distribution*** is a function that gives probabilities of different possible outcomes for a given experiment.
:::

The probability distribution for a discrete random variable, $p(x)$, is called a ***probability mass function (pmf)***.

The probability distribution for a continuous random variable, $f(x)$, is called a ***probability density function (pdf)***.

:::{.example}
Suppose the Colorado Rockies are playing a four game series against the Chicago Cubs and that the Rockies have a 65\% chance of winning an individual game. Further, assume that the games are independent. The following PMF describes the outcomes (number of Rockies wins) and their probabilities.

```{r, echo=F,message=F}
library(tidyverse)
library(kableExtra)
x <- 0:4
px <- c(0.015, 0.111, 0.311, 0.384, 0.179)
df <- t(data.frame(x,px))
row.names(df) = c("Rockies wins, X","Probability, p(X)")
df %>% kable() %>% kable_styling(full_width = F)
```

What is the probability that the Rockies win at least two games?
:::








-----

## Some examples

Over the course of a season, a hockey player scored a goal 30% of the time during a home game, and $P(player\ scores\ |\ away\ game) = .18$. Assume all games are either home or away.

Q: What is the probability the player scored a goal in any game if there were an equal number of home and away games?

A: $P(score) = P(score|home)P(home) + P(score|away)P(away) = .3(.5) + .18(.5) = .24$

Q: What is the probability the player scored a goal in any game if there were twice as many home games as away games?

A: $P(score) = P(score|home)P(home) + P(score|away)P(away) = .3(\frac{2}{3}) + .18(\frac{1}{3}) = .26$

Q: What is the probability the player scored a goal in any game if the ratio of home games to away games is 2:3?

A: $P(score) = P(score|home)P(home) + P(score|away)P(away) = .3(\frac{2}{5}) + .18(\frac{3}{5}) = .228$


### Sets and Conditional Probability

100 sports fans in Colorado were polled and it was found that 64 had attended either a Denver Nuggets or Colorado Avalanche game at Ball Arena (formerly Pepsi Center). 34 people had seen only a Nuggets game, while 17 had seen both a Nuggets and an Avalanche game.

Q: How many people saw an Avalanche game but not a Nuggets game?

A: 64 - 34 - 17 = 13

Q: What is the probability that a randomly selected person in the poll had been to a Nuggets game?

A: (34 + 17) / 100 = .51

Q: What is the probability that a randomly selected person that had been to a game at Ball Arena had been to a Nuggets game?

A: (34 + 17) / 64 = .797

Q: What is the probability that a randomly selected person had been to a Nuggets game given they had been to an Avalanche game?

A: 17 / (17 + 13) = .567


### Binomial Probability

Two baseball teams are playing a 4-game series. The home team has a .65 probability of winning each game, and the away team a .35 probability. Assume each game is independent.

*I used baseball in this example because it's the sport that most often has 4-game series, but it could easily be replaced by another sport.*

Find the following probabilities.

(a) The road team wins exactly 1 game.

$\binom{4}{1}\ .65^3\ .35^1 = \binom{4}{3}\ .65^3\ .35^1 \approx .384$

```{r}
dbinom(1, 4, .35)
dbinom(3, 4, .65)
```

(b) The home team wins exactly 2 games.

$\binom{4}{2}\ .65^2\ .35^2 \approx .311$

```{r}
dbinom(2, 4, .65)
dbinom(2, 4, .35)
```

(c) The road team wins at least 2 games.

$\binom{4}{2}\ .65^2\ .35^2 + \binom{4}{3}\ .65^1\ .35^3 + .35^4 = 1 - [.65^4 + \binom{4}{1}\ .65^3\ .35^1]  \approx .437$

```{r}
pbinom(1.9, 4, .35, lower.tail=F)
pbinom(2, 4, .65, lower.tail=T)

```

(d) The series ends in a sweep.

$.65^4 + .35^4 \approx .194$

```{r}
dbinom(4, 4, .65) + dbinom(4, 4, .35)
.65^4 + .35^4
```



### Binomial Coefficient Symmetry

Playoff series for a certain sports league are played as a best-of-seven series, with one team hosting four games and the opposing team hosing three. An executive for the league wishes to know the number of ways the home and away games can be assigned. (One such combination is A-A-B-B-A-B-A, the format used by the NBA and NHL for their best-of-seven series.) What is the total number of combinations?

Answer: Since there are a fixed number of games (seven) and a fixed number of games that must be given to the lower-seeded team (four), there are $\binom{7}{4} = \frac{7!}{4!\ \cdot\ (7-4)!} = 35$ ways to create a home-away pattern for the seven-game series.

However, instead of thinking about the number of ways to assign the games to the team that gets four home games, what if we thought about the number of ways to assign games to the team that gets three home games?

That would be $\binom{7}{3}$. We can use the `choose` command in R to find this quantity.

```{r}
choose(7,3)
```

It turns out that this binomial coefficient is also equal to 35.

Theorem: $\binom{n}{k} = \binom{n}{n-k}$

$\binom{n}{k} = \frac{n!}{k!\ \cdot\ (n-k)!}$

$\binom{n}{n-k} = \frac{n!}{(n-k)!\ \cdot\ (n-(n-k))!} = \frac{n!}{(n-k)!\ \cdot\ k!} = \binom{n}{k}$


### Binomials and Multinomials

Suppose we are curious about probabilities regarding the results of a soccer team’s next five games.

Wait!!! A soccer game has three possible outcomes (win, lose, draw)! We can’t use the binomial distribution, since it limits us to two possible outcomes!

It depends. If we are interested in the probability that a soccer team wins 2 of their next 5 games, we can use the binomial distribution. We can create the following partition of the sample space of outcomes: $(Win)$ and $(Win^C)$, where the second set includes both losing and drawing.

Then, the formula would be represented as:

$\binom{5}{2}\ P(Win)^2\  P(Win^C)^{(5-2)}$

If we are interested in the probability of the team winning two of the next five games, drawing two, and losing one, we cannot use the binomial theorem. That involves three outcomes, and would be represented as a multinomial. <!--- assuming regularity conditions such as independence. -->


### Geometric (First Success) RVs

Caution: Some references parameterize the Geometric distribution based on the number of failures before the first success, rather than the trial on which the first success occurs. This changes the PMF, mean, and variance, so be careful.

```{r}
set.seed(2022)
geometric <- rgeom(100, 1/3)
head(geometric, 20)
```

Some of the values were 0, which could not happen if R was considering the number of the trial on which the first success occurred. You can add 1 to the values given by `R` to arrive at the First Success distribution.

```{r}
first_success <- geometric + 1
head(first_success, 20)
mean(first_success)
```
The mean of this sample of variables is 3.03, which is close to the expected mean of $\frac{1}{p} = 3$.

### Geometric Distribution - Hockey

Suppose the number of shots needed by a hockey team in order to score their first goal, X, is modeled by a Geometric($\frac{1}{10}$) random variable.

Q: What is the probability that it takes more than 3 shots to score the first goal?

A: $P(X > 3) = P(X = 4) + P(X = 5) + P(X = 6) + ...$

This is an infinite series, so let's use the Law of Total Probability.

$P(X > 3) = 1 - P(X \leq 3) = 1 - [P(X = 1) + P(X = 2) + P(X = 3)] = 1 - [(\frac{1}{10}) + (\frac{9}{10})^1(\frac{1}{10}) + (\frac{9}{10})^2(\frac{1}{10})] = .729$
