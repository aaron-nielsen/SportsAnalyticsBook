# Probability

## Chapter Preview {-}
Simply put, probability is the study of randomness. In this chapter, we will define probability, learn rules of probability, and apply these rules to sports data.

## Definitions

:::{.definition}
An ***experiment*** is any activity or process whose outcome is subject to uncertainty.
:::


:::{.definition}
The ***sample space*** of an experiment, denoted by $\Omega$ or $\mathcal{S}$, is the set of all possible outcomes of that experiment.
:::


:::{.definition}
An ***event*** is any collection (subset) of outcomes contained in the sample space, $\Omega$.
:::

:::{.example}
$$ $$
:::
\
\
\
\
\

:::{.example}
$$ $$
:::
\
\
\
\
\

## Set Theory

For the following examples, suppose that we are interested in the batting outcomes of a plate appearance in softball.

Let $A$ be the event that the batter gets walked, let $B$ be the event that the batter gets a hit, let $C$ be the event that the batter strikes out, and let $D$ be the event that the batter makes it to first base at the end of their at bat.

We will define a handful of set operations to help us when we begin calculating the probability of different events occurring. 

:::{.definition}
The ***compliment*** of an event $A$, denoted by $A^c$ or $A'$, is the set of all outcomes in $\Omega$ that are not contained in $A$.
:::

:::{.example}
Draw a Venn diagram illustrating $A^c$ and describe the event.
:::

\
\
\
\
\

:::{.definition}
The ***union*** of two events $A$ and $B$, denoted by $A \cup B$ and read "$A$ or $B$", is the event consisting of all outcomes that are either in $A$ or $B$ or in both.
:::

:::{.example}
Draw a Venn diagram illustrating $A \cup D$ and describe the event.
:::

\
\
\
\
\

:::{.definition}
The ***intersection*** of two events $A$ and $B$, denoted by $A \cap B$ and read "$A$ and $B$", is the event consisting of all outcomes that are in both $A$ and $B$.
:::

:::{.example}
Draw a Venn diagram illustrating $A \cap D$ and describe the event.
:::

\
\
\
\
\

:::{.definition}
The ***difference*** of two events $A$ and $B$, denoted by $A \mathbin{/} B$ and read "difference of $A$ and $B$", is
the event consisting of all outcomes that are in $A$ but not in $B$.
:::

:::{.example}
Draw a Venn diagram illustrating $D \mathbin{/} A$ and describe the event.
:::

\
\
\
\
\



:::{.definition}
Two events $A$ and $B$ are said to be ***disjoint*** (or ***mutually exclusive***) if $A \cap B = \emptyset$
:::

:::{.example}
Are the events $A$ and $B$ disjoint? How about $A$ and $D$?
:::

\
\
\
\
\




## Probability Axioms and Properties

There are some basic assumptions of "axioms" which are the foundation of the theory of probability. Andrey Kolmogorov first described these axioms in 1933.

### Axioms of Probability
1. $P(A) \geq 0$, for any event $A$ \
2. $P(\Omega) = 1$ \
3. If $A_1, A_2, A_3, \ldots$ is a collection of disjoint events, then: \
$P(\cup_{i=1}^{\infty} A_i) = P(A_1 \cup A_2 \cup \ldots ) = \sum_{i=1}^{\infty} P(A_i)$

Note that all probabilities are between 0 and 1, that is, for any event $A$, $0 \leq P(A) \leq 1$.

We can convert to percentages by multiplying probabilities by 100, however, this is a set that is only done after all calculations have been completed.

### Properties of Probability

- $P(\emptyset) = 0$ \

- $P(A^c) = 1 - P(A)$ 

- $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

- $P(A \cup B \cup C) = \\ P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$

- $P([A \cup B]^c) = P(A^c \cap B^c)$

- $P([A \cap B]^c) = P(A^c \cup B^c)$

## Laws of Probability

:::{.definition}
Let $A$ and $B$ be two events such that $P(B)>0$. Then the ***conditional probability*** of $A$ given $B$, written $P(A|B)$, is given by:
$P(A|B) = \frac{P(A \cap B)}{P(B)}$
:::

:::{ .example}
In 2001, Barry Bonds broke the single season home run record with 73 home runs. In this season, he had 664 plate appearances, 156 hits, 177 walks and 9 hit by pitches. Given that Bonds reached base (via hit, walk, or HBP), what was the probability that he got a hit?
:::

\
\
\

:::{.theorem name="Multiplication Rule"}
For any two events $A$ and $B$, $P(A \cap B) = P(B|A) \cdot P(A)$.
:::

:::{.definition}
Events $A_1, A_2, \ldots, A_n$ are said to form a ***partition*** of a sample space $\Omega$ if both: \
(i) $A_i \cap A_j = \emptyset$ ($i \neq j$) \
(ii) $\cup_{i=1}^n A_i = \Omega$ \
:::


:::{.theorem name="Law of Total Probability"}
Suppose events $A_1, A_2, \ldots, A_n$ form a partition of $\Omega$, then:
$P(B) = P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + \ldots P(B|A_n)P(A_n)$
:::



:::{.theorem name="Bayes Theorem: simple version"}
Suppose events $B$ and $C$ form a partition of $\Omega$, then:
$P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|C)P(C)}$
:::

:::{.theorem name="Bayes Theorem"}
Suppose events $B_1, B_2, \ldots, B_n$ form a partition of $\Omega$, then:
$P(B_k|A) = \frac{P(B_k \cap A)}{P(A)} = \frac{P(A|B_k)P(B_k)}{P(A|B_1)P(B_1)+P(A|B_2)P(B_2) + \ldots + P(A|B_n)P(B_n)}$
:::

:::{.example}
Over the course of a season, a hockey player scored a goal 30% of the time during a home game, and $P(player\ scores\ |\ away\ game) = .18$. Assume all games are either home or away. Use this information to answer the following questions.
:::

(a) What is the probability the player scored a goal in any game if there were an equal number of home and away games? \
\
\
\

(b) What is the probability the player scored a goal in any game if there were twice as many home games as away games? \
\
\
\

(c) What is the probability the player scored a goal in any game if the ratio of home games to away games is 2:3? \
\
\
\

## Combinatorics

Combinatorics is the mathematical study of counting, particularly with respect to permutations and combinations.

:::{.definition}
The ***factorial function ($n!$)*** is defined for all positive integers by: $n! = n \cdot (n-1) \cdot \ldots 2 \cdot 1$
:::

Note that $0! \equiv 1$ and $1! \equiv 1$.

:::{.definition}
An ordered subset is called a ***permutation***. The number of permutations of size $k$ that can be formed from the $n$ elements in a set is given by: $P_{n,k} = \frac{n!}{(n-k)!}$
:::

:::{.definition}
An unordered subset is called a ***combination***. The number of combinations of size $k$ that can be formed from the $n$ elements in a set is given by: $C_{n,k} = {n \choose k} = \frac{n!}{k! \cdot (n-k)!}$
:::

:::{.theorem name="Product Rule for Ordered Pairs"}
If the first element of an ordered pair can be selected in $n_1$ ways and for each of the these $n_1$ ways the second element of the pair can be selected in $n_2$ ways, then the number of pairs is $n_1 \cdot n_2$.
:::

:::{.theorem name="Generalized Product Rule"}
Suppose a set consists of $k$ elements (k-tuples) and that there are $n_1$ possible choices for the first element, $n_2$ possible choices for the second element, ... , and $n_k$ possible choices for the $k^\text{th}$ element, then there are $n_1 \cdot n_2 \cdot \ldots \cdot n_k$ possible k-tuples.
:::

## Odds and Gambling


## Random Variables

:::{.definition}
Let $\Omega$ be the sample space of an experiment. A ***random variable*** is a rule that associates a number with each outcome in $\Omega$. In other words, a random variable is a function whose domain is $\Omega$ and whose range is the set of real numbers.
:::

Random variables are be broken down into subcategories:\
1. ***Discrete random variables*** - random variables which have a sample space that is finite or countably infinite.\
2. ***Continuous random variables*** - random variables which have a sample space that is uncountably infinite (such as an interval of real numbers)

***Discrete*** and ***Continuous*** random variables use similar yet slightly different mathematical tools. Discrete random variables involve working with "sums" and continuous random variables involve working with "integrals".

:::{.example}
$$ $$
:::

\
\
\

:::{.example}
$$ $$
:::

\
\
\

:::{.definition}
A ***probability distribution*** is a function that gives probabilities of different possible outcomes for a given experiment.
:::

The probability distribution for a discrete random variable, $p(x)$, is called a ***probability mass function (pmf)***.

The probability distribution for a continuous random variable, $f(x)$, is called a ***probability density function (pdf)***.

:::{.example}
Suppose the Colorado Rockies are playing a four game series against the Chicago Cubs and that the Rockies have a 65\% chance of winning an individual game. Further, assume that the games are independent. The following PMF describes the outcomes (number of Rockies wins) and their probabilities.

```{r, echo=F,message=F}
library(tidyverse)
library(kableExtra)
x <- 0:4
px <- c(0.015, 0.111, 0.311, 0.384, 0.179)
df <- t(data.frame(x,px))
row.names(df) = c("Rockies wins, X","Probability, p(X)")
df %>% kable() %>% kable_styling(full_width = F)
```

What is the probability that the Rockies win zero games? What is the probability that the Rockies win at least two games? Why might the independence assumption be false?
:::

\
\
\
\
\

We may be interested in describing the center or average value of our random variable. We can do this with the following definitions.

:::{.definition}
The ***expected value*** (or ***population mean*** or ***average***) of a random variable $X$ is given by: \

(i) $E[X] = \mu = \sum_{x \in \Omega} x \cdot p(x)$ (for discrete random variables)

(ii) $E[X] = \mu = \int_{x \in \Omega} x \cdot f(x) dx$ (for continuous random variables) 

:::

For this class, evaluating integrals is not essential, so we will avoid using Calculus (integrals and derivatives) when possible.

Sometimes, it makes sense to calculate the expected value of a function of a random variable. This can be easily done with a slight modification to the previous definition. Let $h(X)$ be some function of a random variable $X$. The expected value of $h(X)$, $E[h(X)]$, is given by:

(i) $E[h(X)] = \sum_{x \in \Omega} h(x) \cdot p(x)$ (for discrete random variables)

(ii) $E[h(X)] = \int_{x \in \Omega} h(x) \cdot f(x) dx$ (for continuous random variables) 

:::{.example}
For the Rockies/Cubs four game series example, calculate $E[X]$ and $E[X^2]$.
:::

\
\
\
\
\


The spread or variability associated with a random variable can be calculated using expected values as well.

:::{.definition}
The ***population variance*** of a random variable $X$ is given by: \

(i) $Var(X) = \sum_{x \in \Omega} (x-\mu)^2 \cdot p(x)$ (for discrete random variables)

(ii) $Var(X) = \int_{x \in \Omega} (x-\mu)^2 \cdot f(x) dx$ (for continuous random variables) 

:::


There is also a shortcut formula for calculating variance: \

:::{.theorem}

$Var(X) = E[X^2] - (E[X])^2$

:::

:::{.definition}

The ***population standard deviation*** of a random variable $X$ is given by: \

$SD(X) = \sigma = \sqrt{Var(X)} = \sqrt{E[X^2]-(E[X])^2}$
:::

:::{.example}
For the Rockies/Cubs four game series example, calculate $Var(X)$.
:::

\
\
\
\
\

## Common Random Variables

There are several families of random variables that show up frequently in applications. Some of these random variables include:
- Binomial
- Geometric
- Poisson
- Normal

### Binomial RVs

:::{.definition}
A ***binomial(n,p) random variable*** is a discrete random variable that counts the numbers of "successes" over a fixed number of trials, $n$, with each trial having an equal probability of success, $p$.

$P(X=k) = \binom{n}{k} p^k(1-p)^{n-k} = \frac{n!}{k!\ \cdot\ (n-k)!} p^k(1-p)^{n-k}$, where $0 \leq k \leq n, 0 \leq p \leq 1$

If $X \sim Binomial(n,p)$, then $E[X]=np$ and $Var(X)=np(1-p)$
:::

:::{.example}
The Cubs and Rockies are playing a 4-game series. The Rockies have a 0.65 probability of winning each game, and the Cubs have a 0.35 probability. Assume each game is independent. Solve for the following quantities.
:::

(a) The Cubs wins exactly 1 game. \
\
\
\

(b) The Rockies win exactly 2 games.\
\
\
\

(c) The Cubs win at least 2 games.\
\
\
\

(d) The series ends in a sweep. \
\
\
\

(e) The expected number of wins for the Rockies.\
\
\
\

(f) The variance and standard deviations of wins for the Rockies.\
\
\
\

:::{.example}
Complete 10,000 simulations of the four game series between the Rockies and Cubs. For the number of Rockies wins, calculate the sample mean and sample variance and compare these to the population values. Also, plot a histogram of the sample data.
:::

```{r}
set.seed(2020)
rockies_wins <- rbinom(n=10000,size=4,prob=0.65)
mean(rockies_wins)
var(rockies_wins)
rockies_wins_df <- data.frame(Wins=rockies_wins)
rockies_wins_df %>% ggplot(aes(Wins)) + geom_histogram(binwidth = 1,color = "black", fill = "purple")
```

#### Binomial Coefficient Symmetry

Playoff series for a certain sports league are played as a best-of-seven series, with one team hosting four games and the opposing team hosing three. An executive for the league wishes to know the number of ways the home and away games can be assigned. (One such combination is A-A-B-B-A-B-A, the format used by the NBA and NHL for their best-of-seven series.) What is the total number of combinations?\
\
\
\

However, instead of thinking about the number of ways to assign the games to the team that gets four home games, what if we thought about the number of ways to assign games to the team that gets three home games?

That would be $\binom{7}{3}$. We can use the `choose` command in R to find this quantity.

```{r}
choose(7,3)
```

It turns out that this binomial coefficient is also equal to 35.

Theorem: $\binom{n}{k} = \binom{n}{n-k}$

$\binom{n}{k} = \frac{n!}{k!\ \cdot\ (n-k)!}$

$\binom{n}{n-k} = \frac{n!}{(n-k)!\ \cdot\ (n-(n-k))!} = \frac{n!}{(n-k)!\ \cdot\ k!} = \binom{n}{k}$

### Geometric RVs

:::{.definition}
A ***Geometric(p) random variable*** is a discrete random variable that counts the numbers of trials until a "success" occurs, where the probability of success, $p$, is constant across all trials.

$P(X=k) = p(1-p)^{k-1}$, where $k \geq 1, 0 \leq p \leq 1$

If $X \sim Geometric(p)$, then $E[X]=\frac{1}{p}$ and $Var(X)=\frac{p}{1-p}$
:::


:::{.example}
Suppose the number of shots needed by a hockey team in order to score their first goal, X, is modeled by a Geometric($\frac{1}{10}$) random variable. Use this information to answer the following questions.
:::

(a) What is the probability that it takes exactly 3 shots to score the first goal? \
\
\
\

(b) What is the probability that it takes less than 3 shots to score the first goal? \
\
\
\

(c) What is the probability that it takes more than 3 shots to score the first goal? \
\
\
\

**Caution:** Some references parameterize the Geometric distribution based on the number of failures before the first success, rather than the trial on which the first success occurs. This changes the PMF, mean, and variance, so be careful.

Let's simulate the number of shot attempts required to score the first goal (Geometric($p=1/10$)) from the previous example.

```{r}
set.seed(2020)
geometric <- rgeom(1000, 1/10)
head(geometric, 20)
```

Some of the values were 0, which could not happen if R was considering the number of the trial on which the first success occurred. You can add 1 to the values given by `R` to arrive at the first success distribution.

```{r}
first_success <- geometric + 1
head(first_success, 20)
mean(first_success)
```
The mean of this sample of variables is 10.827, which is close to the expected mean of $\frac{1}{p} = 10$.

Let's plot the sample distribution of shots required to score a goal from the simulation as well.

```{r}
first_success_df = data.frame(Shots = first_success)
first_success_df %>% ggplot(aes(x=Shots)) + geom_histogram(binwidth = 1)
```

### Poisson RVs

:::{.definition}
A ***Poisson($\lambda$) random variable*** is a discrete random variable that counts the numbers of "successes" for a given rate parameter, $\lambda$, for a given interval.

$P(X=k) =  \frac{e^{-\lambda}\lambda^k}{k!}$, where $k \geq 1,$

If $X \sim Poisson(\lambda)$, then $E[X]=\lambda$ and $Var(X)=\lambda$
:::

:::{.example}
During the 2021 Major League Soccer season, the Colorado Rapids scored 51 goals in 34 games on their way to a first-place finish in the Western Conference regular season standings.

The team scored $\frac{51}{34} = 1.5$ goals per game. Let's model the distribution of Rapids goals using a Poisson(1.5) random variable that we'll call Y.
:::

(a) Which is more likely: Y taking on the value 0 or Y taking on the value 2?\
\
\
\

We can plot the PMF of Y to check visually.

```{r}
ggplot(transform(data.frame(x=c(0:8)), y=dpois(x, lambda = 1.5)), aes(x, y)) + 
  geom_bar(stat="identity") + 
  labs(x="Value", y="Frequency", title="Probability mass function of Poisson(1.5) random variable")
```


We can calculate these probabilities in R as well using the `dpois` command.

```{r}
dpois(x=0, lambda=1.5)
dpois(x=2, lambda=1.5)
```


Let's check whether using a Poisson distribution was appropriate by comparing it to the actual 2021 Colorado Rapids match results.

```{r}
# Data: https://www.espn.com/soccer/team/results/_/id/184/season/2021

library("kableExtra")

goals <- c(0:4, "5 or more")
actual_frequency <- c(6, 14, 7, 6, 0, 1)
actual_proportion <- actual_frequency / sum(actual_frequency)
expected_proportion <- c(dpois(0:4, lambda=1.5), ppois(4, lambda=1.5, lower.tail=FALSE))
expected_frequency <- round(expected_proportion * 34, 1)

rapids.data <- data.frame(goals, actual_frequency, actual_proportion, expected_frequency, expected_proportion)

rapids.data %>%
kbl() %>%
kable_styling()
```

(b) What differences do you notice between the actual results and the expected values based on the Poisson random variable? \
\
\
\

<!-- There were fewer games in which the Rapids scored 4 or more goals than the model would indicate, yet the Rapids were shut out less often than the model would indicate. -->

(c) Even if the true population distribution of 2021 Rapids goals was truly a Poisson(1.5) random variable, why might the actual distribution of their goals differ from the probability mass function? \
\
\
\

<!-- 34 is a relatively small sample size; random variables may not coincide with their expected values for finite sample sizes. -->

(d) What are the advantages of using the Poisson distribution to model Major League soccer goals? What are the disadvantages? \
\
\
\

<!-- Poisson random variables can take on the natural numbers (including zero), which aligns with the number of goals that can be scored in a match. One disadvantage is that it is possible for a Poisson to take on values that are not realistic for the situation, such as double-digit integers or higher. Only one game in MLS history has had a team score more than seven goals in a game. However, when $\lambda$ is small (such as 1.5), these extreme values are relatively unlikely. -->


### Normal RVs

:::{.definition}
A ***Normal($\mu$,$\sigma^2$) random variable*** is a continuous random variable that is bell-shaped with mean $\mu$ and variance $\sigma^2$.

To calculate probabilities under the normal curve, you need either to integrate, use a table, or a computer.

Note that a normal random variable can be standardized by using: $z = \frac{x-\mu}{\sigma}$
:::

:::{.theorem}
For a normal($\mu$,$\sigma^2$) random variable, we have the following approximations: \
- About 68\% of the data falls within one standard deviation of the mean (i.e., $\mu \pm \sigma$) \
- About 95\% of the data falls within two standard deviations of the mean (i.e., $\mu \pm 2\sigma$) \
- About 99.7\% of the data falls within three standard deviations of the mean (i.e., $\mu \pm 3\sigma$)
:::

:::{.example}
The skills (or tools) of a baseball player are often rated on a scale of 20-80, where 50 is an average grade, 20 is the lowest grade, and 80 is the highest grade. The distribution of tool grades is approximately normally distributed ($\mu=50, \sigma =10$).

See [https://blogs.fangraphs.com/scouting-explained-the-20-80-scouting-scale/](https://blogs.fangraphs.com/scouting-explained-the-20-80-scouting-scale/) for more details. Calculate the following probabilities.
:::

(a) Former Rockie Nolan Arenado has been graded to have game power of 70. Game power estimates a player's ability to hit home runs. Approximately what percentage of baseball players have equal or greater game power than Arenado? \
\
\
\

(b) Mike Trout has been graded to have raw power of 55. Raw power estimates a player's ability to hit baseballs hard (i.e., hard hit rate). Approximately what percentage of baseball players have equal or less raw power than Arenado? \
\
\
\

(c) Suppose a Rockies prospect is said to be in the top 10\% of all baseball players in terms of their speed. What approximate speed grade would correspond to the player? \
\
\
\

(d) Suppose a Rockies prospect is said to be in the bottom 20\% of all baseball players in terms of their hit ability. What approximate hit grade would correspond to the player? \
\
\
\


(e) Between what two grades do approximately 95\% of all players lie for a given tool? \
\
\
\

Let's check our answers:
```{r}
a <- 1-pnorm(q=70,mean=50,sd=10); a
b <- pnorm(q=55,mean=50,sd=10); b
c <- qnorm(0.1,mean=50,sd=10,lower.tail = F); c
d <- qnorm(0.2,mean=50,sd=10,lower.tail = T); d
e <- pnorm(q=70,mean=50,sd=10) - pnorm(q=30,mean=50,sd=10); e
```




-----

## Extra Stuff




### Sets and Conditional Probability

100 sports fans in Colorado were polled and it was found that 64 had attended either a Denver Nuggets or Colorado Avalanche game at Ball Arena (formerly Pepsi Center). 34 people had seen only a Nuggets game, while 17 had seen both a Nuggets and an Avalanche game.

Q: How many people saw an Avalanche game but not a Nuggets game?

A: 64 - 34 - 17 = 13

Q: What is the probability that a randomly selected person in the poll had been to a Nuggets game?

A: (34 + 17) / 100 = .51

Q: What is the probability that a randomly selected person that had been to a game at Ball Arena had been to a Nuggets game?

A: (34 + 17) / 64 = .797

Q: What is the probability that a randomly selected person had been to a Nuggets game given they had been to an Avalanche game?

A: 17 / (17 + 13) = .567


### Binomials and Multinomials

Suppose we are curious about probabilities regarding the results of a soccer team’s next five games.

Wait!!! A soccer game has three possible outcomes (win, lose, draw)! We can’t use the binomial distribution, since it limits us to two possible outcomes!

It depends. If we are interested in the probability that a soccer team wins 2 of their next 5 games, we can use the binomial distribution. We can create the following partition of the sample space of outcomes: $(Win)$ and $(Win^C)$, where the second set includes both losing and drawing.

Then, the formula would be represented as:

$\binom{5}{2}\ P(Win)^2\  P(Win^C)^{(5-2)}$

If we are interested in the probability of the team winning two of the next five games, drawing two, and losing one, we cannot use the binomial theorem. That involves three outcomes, and would be represented as a multinomial. <!--- assuming regularity conditions such as independence. -->



### Expectation - Baseball

The expectation of a discrete random variable is a weighted average. The "weights" are the probabilities of the possible values of the variable.

Consider the following table, which shows the number of career hits by type for the all-time Major League Baseball leader in total bases, Hank Aaron.

<!-- https://www.baseball-reference.com/players/a/aaronha01.shtml -->

```{r expectation baseball, echo=F}
Hit_type <- c("Single", "Double", "Triple", "Home Run")
Number_bases <- c(1:4)
Hit_Frequency <- c(2294, 624, 98, 755)
Hit_Proportion <- round(Hit_Frequency / sum(Hit_Frequency), 8)

Hank_Aaron_Hits <- data.frame(Hit_type, Number_bases, Hit_Frequency, Hit_Proportion)

Hank_Aaron_Hits %>%
kbl() %>%
kable_styling()
```

The expected number of bases for a Hank Aaron hit is the sum of the number of bases attained for each hit multiplied by the relative frequency of the occurrence of that type of hit.

$1 \cdot \frac{2294}{3771} + 2 \cdot \frac{624}{3771} + 3 \cdot \frac{98}{3771} + 4 \cdot \frac{755}{3771} = 1.18181$

This is the same process that is occurring whenever we calculate the expectation of any discrete random variable. Recall the formula for expectation is $E[X] = \sum_{x \in \Omega}\ x \cdot p(x)$. Each value in the sample space is "adjusted" by the probability of that value, then the sum of all values in $\Omega$ is taken to arrive at the weighted average, or expected value, of the random variable.

### Basketball Scenario

You are the coach of a basketball team that is down two points with one second remaining in the fourth quarter. During a timeout, you are considering the best play to call for your team. The first option is a three-point shot attempt, which you estimate has a 30% chance of succeeding. The second option is a two-point shot attempt, which has a 50% chance of making the field goal, a 30% chance of missing it and ending the game, and a 20% chance the shooter will miss but be fouled, in which case the shooter's free throw success will follow a $Bin(2, .8)$ random variable. Finally, you estimate that your team's probability of winning the game in overtime is .45.

Assume the above situations are exhaustive (i.e., the other team will not get another possession, no fouls will be called before the ball is put in play, lightning will not hit the arena and postpone the game, etc.). Which of the two plays should you call to maximize the win probability for your team?

A: The probability of winning the game with the three-point shot attempt is .3. If the two-point shot attempt is called for, there is a .5 probability of making the field goal and a (.2)(.8)(.8) = .128 probability that the foul is called and both free throws are made. Thus, the total probability of scoring two points and sending the game to overtime is .628. Then, the probability of winning the game in OT after tying it in regulation is (.628)(.45) = .2828. This is less than .3, so shooting the three-pointer is the option that maximizes the win probability, given these situational probabilities.

Q: What is the minimum estimated overtime win probability to make calling for the two-point play the better option?

A: $P(score\ 2\ points\ in\ regulation) \cdot P(win\ in\ OT) > P(win\ in\ regulation)$\
$.628 \cdot P(win\ in\ OT) > .3$\
$P(win\ in\ OT) > .478$

### Multiple Probability Distributions - Basketball

Suppose the number of points scored by a basketball player follows a Poisson(12) random variable, the number of rebounds by a Poisson(7) distribution, and assists by a Discrete Uniform(2, 11), independently of each other.

Q: What is the probability that this player records a points, rebounds, assists triple-double in a game?

A: $P(Triple\ Double) = P(Points \geq 10\ \cap\ Rebounds \geq 10\ \cap\ Assists \geq 10)$

```{r basketball probability 1}
ppois(9, lambda = 12, lower.tail=F)
```

$P(Points \geq 10) = P(Poisson(12) \geq 10) \approx .758$

```{r basketball probability 2}
ppois(9, lambda = 7, lower.tail=F)
```

$P(Rebounds \geq 10) = P(Poisson(7) \geq 10) \approx .170$

$P(Assists \geq 10) = P(Discrete\ Uniform(2, 11) \geq 10) = .2$

Since the events are independent, we can multiply their probabilities. The probability of the player scoring the triple-double is $(.758)(.170)(.2) = .0257$.

Q: Your friend offers you 4 to 1 that the player will not record a triple-double in their next 10 games. With the knowledge that the athlete's performance in a game is unaffected by performances in previous games, would you take the bet?

A: $P(no\ triple\ double) = 1 - .0257 = .9743$, so $P(no\ triple\ double\ in\ next\ 10\ games) = (.9743)^{10} = .771$

The odds of no triple-double are $\frac{.771}{1-.771} = 3.37$, so the bet of no triple-double at 4 to 1 odds is favorable.



*answers may vary for following questions*

Q: What differences do you notice between the actual results and the expected values based on the Poisson random variable?

A: There were fewer games in which the Rapids scored 4 or more goals than the model would indicate, yet the Rapids were shut out less often than the model would indicate.

Q: Even if the true population distribution of 2021 Rapids goals was truly a Poisson(1.5) random variable, why might the actual distribution of their goals differ from the probability mass function?

A: 34 is a relatively small sample size; random variables may not coincide with their expected values for finite sample sizes.

Q: What are the advantages of using the Poisson distribution to model Major League soccer goals? What are the disadvantages?

A: Poisson random variables can take on the natural numbers (including zero), which aligns with the number of goals that can be scored in a match. One disadvantage is that it is possible for a Poisson to take on values that are not realistic for the situation, such as double-digit integers or higher. Only one game in MLS history has had a team score more than seven goals in a game. However, when $\lambda$ is small (such as 1.5), these extreme values are relatively unlikely.


### Law of Total Probability - Hockey

Over the course of a season, a hockey player scored a goal 30% of the time during a home game, and $P(player\ scores\ |\ away\ game) = .18$. Assume all games are either home or away.

Q: What is the probability the player scored a goal in any game if there were an equal number of home and away games?

A: $P(score) = P(score|home)P(home) + P(score|away)P(away) = .3(.5) + .18(.5) = .24$

Q: What is the probability the player scored a goal in any game if there were twice as many home games as away games?

A: $P(score) = P(score|home)P(home) + P(score|away)P(away) = .3(\frac{2}{3}) + .18(\frac{1}{3}) = .26$

Q: What is the probability the player scored a goal in any game if the ratio of home games to away games is 2:3?

A: $P(score) = P(score|home)P(home) + P(score|away)P(away) = .3(\frac{2}{5}) + .18(\frac{3}{5}) = .228$

### Law of Total Probability - Baseball

You work in the front office of a professional baseball club and have just learned that a certain prospect hits .200 against left-handed pitchers and .400 against right-handed pitchers (their overall batting average is unknown). The general manager of the team overhears you talking about the .400 statistic of the player and becomes very exited that they have the chance to draft a .400 hitter. What would you say to caution the GM that the player might not be a remarkable hitter?

A: We don't know the proportion of the player's at-bats that came against left-handed pitchers versus right-handed pitchers. If we want to know the player's batting average unconditional on the type of pitcher they are facing, we have to adjust $P(hit\ |\ left-handed\ pitcher)$ by $P(left-handed\ pitcher)$ and $P(hit\ |\ right-handed\ pitcher)$ by $P(right-handed\ pitcher)$ before adding them to determine $P(hit)$. For example, if 90% of the player's at-bats were against left-handed pitchers, then their overall batting average is a pedestrian .220.

*Other possible issues: low sample size of player's at-bats, the fact that pro pitchers will be harder to hit against than non-pros*


