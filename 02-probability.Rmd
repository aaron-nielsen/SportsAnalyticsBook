---
output:
  pdf_document: default
  html_document: default
---

```{r,echo=FALSE,message=FALSE,warning=FALSE}
library(formatR)
library(tidyverse)
library(kableExtra)

# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=FALSE)
knitr::opts_chunk$set(cache=TRUE)

# kable table global setup
kt <- function(data) {
  knitr::kable(data, digits=3,linesep='',booktabs=TRUE) %>% kable_styling(bootstrap_options='striped', latex_options='HOLD_position', full_width = F, position = "center")
}
```

# Probability

## Definitions and Set Theory

:::{.definition}
An ***experiment*** is any activity or process whose outcome is subject to uncertainty.
:::


:::{.definition}
The ***sample space*** of an experiment, denoted by $\Omega$ or $\mathcal{S}$, is the set of all possible outcomes of that experiment.
::: 


:::{.definition}
An ***event*** is any collection (subset) of outcomes contained in the sample space, $\Omega$.
:::

:::{.example}
Give some examples of discrete random variables in sports.
:::
\
\
\
\
\

:::{.example}

Give some examples of continuous random variables in sports.
:::
\ 
\
\
\
\

\newpage


For the following examples, suppose that we are interested in the batting outcomes of a plate appearance in baseball.

Let $A$ be the event that the batter gets walked, let $B$ be the event that the batter gets a hit, let $C$ be the event that the batter strikes out, and let $D$ be the event that the batter makes it to first base at the end of their at bat.

We will define a handful of set operations to help us when we begin calculating the probability of different events occurring. 

:::{.definition} 
The ***compliment*** of an event $A$, denoted by $A^c$ or $A'$, is the set of all outcomes in $\Omega$ that are not contained in $A$.
:::

:::{.example}
Draw a Venn diagram illustrating $A^c$ and describe the event.
:::

\
\
\
\
\
\vfill

:::{.definition}
The ***union*** of two events $A$ and $B$, denoted by $A \cup B$ and read "$A$ or $B$", is the event consisting of all outcomes that are either in $A$ or $B$ or in both.
:::

:::{.example}
Draw a Venn diagram illustrating $A \cup D$ and describe the event.
:::

\
\
\
\
\
\vfill

\newpage

:::{.definition}
The ***intersection*** of two events $A$ and $B$, denoted by $A \cap B$ and read "$A$ and $B$", is the event consisting of all outcomes that are in both $A$ and $B$.
:::

:::{.example}
Draw a Venn diagram illustrating $A \cap D$ and describe the event.
:::

\
\
\
\
\
\vfill

:::{.definition}
The ***difference*** of two events $A$ and $B$, denoted by $A \mathbin{/} B$ and read "difference of $A$ and $B$", is
the event consisting of all outcomes that are in $A$ but not in $B$.
:::

:::{.example}
Draw a Venn diagram illustrating $D \mathbin{/} A$ and describe the event.
:::

\
\
\
\
\
\vfill



:::{.definition}
Two events $A$ and $B$ are said to be ***disjoint*** (or ***mutually exclusive***) if $A \cap B = \emptyset$
:::

:::{.example}
Are the events $A$ and $B$ disjoint? How about $A$ and $D$?
:::

\
\
\
\
\
\vfill

\newpage




## Axioms, Properties, and Laws

There are some basic assumptions or "axioms" which are the foundation of the theory of probability. Andrey Kolmogorov first described these axioms in 1933.

### Axioms of Probability
1. $P(A) \geq 0$, for any event $A$ \
2. $P(\Omega) = 1$ \
3. If $A_1, A_2, A_3, \ldots$ is a collection of disjoint events, then: \
$P(\cup_{i=1}^{\infty} A_i) = P(A_1 \cup A_2 \cup \ldots ) = \sum_{i=1}^{\infty} P(A_i)$ \

Note that all probabilities are between 0 and 1, that is, for any event $A$, $0 \leq P(A) \leq 1$.

We can convert to percentages by multiplying probabilities by 100, however, this is a set that is only done after all calculations have been completed.

### Properties of Probability

- $P(\emptyset) = 0$ \

- $P(A^c) = 1 - P(A)$ \

- $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ \

- $P(A \cup B \cup C) = \\ P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$ \

- $P([A \cup B]^c) = P(A^c \cap B^c)$ \

- $P([A \cap B]^c) = P(A^c \cup B^c)$ \

\newpage

:::{.example}
In 2001, Barry Bonds broke the single season home run record with 73 home runs. In this season, he had 664 plate appearances (476 at-bats), 156 hits, 177 walks, 9 hit by pitches, and 2 sacrifice flies. Use this information to answer the following questions.
:::

(a) Plate appearances that result in a walk, hit by pitch, or sacrifice fly are not counted towards a player's at-bats. Confirm that Bonds had 476 official at-bats. \
\
\
\
\vfill

(b) Suppose an at-bat is chosen at random. What is the probability that Bonds got a hit? (This is his batting average.) \
\
\
\
\vfill


(c) Suppose a plate appearance is chosen at random. What is the probability that Bonds reached base via a hit, walk, or hit by pitch? (This is his on-base average/percentage.) \
\
\
\
\vfill

(d) What is the probability that Bonds did not reach base? \
\
\
\
\vfill


\newpage


### Laws of Probability

:::{.definition}
Let $A$ and $B$ be two events such that $P(B)>0$. Then the ***conditional probability*** of $A$ given $B$, written $P(A|B)$, is given by:
$P(A|B) = \frac{P(A \cap B)}{P(B)}$
:::

:::{.example}
The following contingency table summarizes complete games by league during the 2012 MLB season. Use the contingency table to answer the following questions regarding probability. \
:::

```{r,echo=F}
League <- c(rep(0,2592),rep(1,2268))
Complete <- rep(0,4860)
Complete[1:59] = 1
Complete[2593:(2593+68)]=1
mlb_corr <- data.frame(League,Complete)
mlb_corr_table <- table(mlb_corr)
rownames(mlb_corr_table) = c("NL","AL")
colnames(mlb_corr_table) = c("No","Yes")
mlb_corr_table <- mlb_corr_table[,c(2,1)]
mlb_corr_table %>% kable(booktabs=T,digits = 3)
```

\
\

(a) $P(\text{Complete Game})$ \
\vfill

(b) $P(\text{Complete Game | NL})$ \
\vfill

(c) $P(\text{Complete Game | AL})$ \
\vfill

(d) $P(\text{Complete Game } \cup \text{AL})$ \
\vfill

(e) $P(\text{Complete Game } \cap \text{AL})$ \
\vfill



\newpage


<!-- :::{.theorem name="Multiplication Rule"} -->
<!-- For any two events $A$ and $B$, $P(A \cap B) = P(B|A) \cdot P(A)$. -->
<!-- ::: -->

<!-- :::{.example} -->
<!-- Calculate the probability that a randomly selected plate appearance from Sosa's 1998 season is a home run in June. -->
<!-- ::: -->

<!-- \vfill -->




<!-- :::{.definition} -->
<!-- Events $A_1, A_2, \ldots, A_n$ are said to form a ***partition*** of a sample space $\Omega$ if both: \ -->
<!-- (i) $A_i \cap A_j = \emptyset$ ($i \neq j$) \ -->
<!-- (ii) $\cup_{i=1}^n A_i = \Omega$ \ -->
<!-- ::: -->


<!-- :::{.theorem name="Law of Total Probability"} -->
<!-- Suppose events $A_1, A_2, \ldots, A_n$ form a partition of $\Omega$, then: -->
<!-- $P(B) = P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + \ldots P(B|A_n)P(A_n)$ -->
<!-- ::: -->

<!-- :::{.example} -->
<!-- What is one possible way to partition Sosa's plate appearances in 1998? -->
<!-- ::: -->

<!-- \vfill -->



<!-- :::{.theorem name="Bayes Theorem: simple version"} -->
<!-- Suppose events $B$ and $C$ form a partition of $\Omega$, then: -->
<!-- $P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|C)P(C)}$ -->
<!-- ::: -->

<!-- :::{.theorem name="Bayes Theorem"} -->
<!-- Suppose events $B_1, B_2, \ldots, B_n$ form a partition of $\Omega$, then: -->
<!-- $P(B_k|A) = \frac{P(B_k \cap A)}{P(A)} = \frac{P(A|B_k)P(B_k)}{P(A|B_1)P(B_1)+P(A|B_2)P(B_2) + \ldots + P(A|B_n)P(B_n)}$ -->
<!-- ::: -->

<!-- :::{.example} -->
<!-- Given that Sosa hit a home run in 1998, what is the probability that he hit it in June? -->
<!-- ::: -->

<!-- \vfill -->

<!-- \newpage -->


<!-- :::{.example} -->
<!-- Over the course of a season, a hockey player scored a goal 30% of the time during a home game, and 18%. Assume all games are either home or away. Use this information to answer the following questions. -->
<!-- ::: -->

<!-- (a) What is the probability the player scored a goal in any game if there were an equal number of home and away games? \ -->

<!-- \vfill -->

<!-- (b) What is the probability the player scored a goal in any game if there were twice as many home games as away games? \ -->

<!-- \vfill -->

<!-- (c) What is the probability the player scored a goal in any game if the ratio of home games to away games is 2:3? \ -->

<!-- \vfill -->


\newpage

## Combinatorics

Combinatorics is the mathematical study of counting, particularly with respect to permutations and combinations.

:::{.definition}
The ***factorial function ($n!$)*** is defined for all positive integers by: $n! = n \cdot (n-1) \cdot \ldots 2 \cdot 1$
:::

Note that $0! \equiv 1$ and $1! \equiv 1$.

:::{.example}
A baseball/softball batting lineup has nine ordered players. Suppose the manager has selected the nine players to bat. How many different batting orders are there possible?
:::

\vfill

:::{.definition}
An ordered subset is called a ***permutation***. The number of permutations of size $k$ that can be formed from the $n$ elements in a set is given by: $P_{n,k} = \frac{n!}{(n-k)!}$
:::

:::{.example}
In MLB, each team has a 26-person roster, of which about 13 are hitters. Assuming one of the batters is the designated hitter, how many different batting lineups of 9 players can a manager create?
:::

\vfill

:::{.definition}
An unordered subset is called a ***combination***. The number of combinations of size $k$ that can be formed from the $n$ elements in a set is given by: $C_{n,k} = {n \choose k} = \frac{n!}{k! \cdot (n-k)!}$
:::

:::{.example}
Suppose a manager has 13 hitters to choose from to fill out a starting lineup of 9 players. Ignoring positions, how many different ways can the manager pick a 9-person lineup from 13 possible hitters?
:::

\vfill

\newpage

:::{.theorem name="Product Rule for Ordered Pairs"}
If the first element of an ordered pair can be selected in $n_1$ ways and for each of the these $n_1$ ways the second element of the pair can be selected in $n_2$ ways, then the number of pairs is $n_1 \cdot n_2$.
:::


:::{.theorem name="Generalized Product Rule"}
Suppose a set consists of $k$ elements (k-tuples) and that there are $n_1$ possible choices for the first element, $n_2$ possible choices for the second element, ... , and $n_k$ possible choices for the $k^\text{th}$ element, then there are $n_1 \cdot n_2 \cdot \ldots \cdot n_k$ possible k-tuples.
:::

:::{.example}
A baseball manager selects nine hitters and one pitcher for a starting lineup. Suppose they can choose from 13 hitters and 13 hitters. How many different possible lineups can the manager choose from?
:::

\vfill

\newpage

## Random Variables

:::{.definition}
Let $\Omega$ be the sample space of an experiment. A ***random variable*** is a rule that associates a number with each outcome in $\Omega$. In other words, a random variable is a function whose domain is $\Omega$ and whose range is the set of real numbers.
::: 

Random variables are be broken down into subcategories:\
1. ***Discrete random variables*** - random variables which have a sample space that is finite or countably infinite.\
2. ***Continuous random variables*** - random variables which have a sample space that is uncountably infinite (such as an interval of real numbers) \

***Discrete*** and ***Continuous*** random variables use similar yet slightly different mathematical tools. Discrete random variables involve working with "sums" and continuous random variables involve working with "integrals". \

:::{.example}
Give an example of a discrete random variable in hockey.
:::

\
\
\
\vfill

:::{.example}
Give an example of a continuous random variable in hockey.
:::

\
\
\
\vfill

:::{.definition}
A ***probability distribution*** is a function that gives probabilities of different possible outcomes for a given experiment.
:::

The probability distribution for a discrete random variable, $p(x)$, is called a ***probability mass function (pmf)***.

The probability distribution for a continuous random variable, $f(x)$, is called a ***probability density function (pdf)***.

\newpage

:::{.example}
Suppose the Colorado Rockies are playing a four game series against the Chicago Cubs and that the Rockies have a 65\% chance of winning an individual game. Further, assume that the games are independent. The following PMF describes the outcomes (number of Rockies wins) and their probabilities.

```{r, echo=F}
x <- 0:4
px <- c(0.015, 0.111, 0.311, 0.384, 0.179)
df <- t(data.frame(x,px))
row.names(df) = c("Rockies wins, X","Probability, p(X)")
df %>% kt()
```

\vfill

(a) What is the probability that the Rockies win zero games?\
\vfill

(b) What is the probability that the Rockies win at least two games?\
\vfill

(c) Why might the independence assumption be false?\
\vfill
:::

\newpage

We may be interested in describing the center or average value of our random variable. We can do this with the following definitions.

:::{.definition}
The ***expected value*** (or ***population mean*** or ***average***) of a random variable $X$ is given by: 

(i) $E[X] = \mu = \sum_{x \in \Omega} x \cdot p(x)$ (for discrete random variables)

(ii) $E[X] = \mu = \int_{x \in \Omega} x \cdot f(x) dx$ (for continuous random variables) 

:::

For this class, evaluating integrals is not essential, so we will avoid using Calculus (integrals and derivatives) when possible.

Sometimes, it makes sense to calculate the expected value of a function of a random variable. This can be easily done with a slight modification to the previous definition. Let $h(X)$ be some function of a random variable $X$. The expected value of $h(X)$, $E[h(X)]$, is given by:

(i) $E[h(X)] = \sum_{x \in \Omega} h(x) \cdot p(x)$ (for discrete random variables)

(ii) $E[h(X)] = \int_{x \in \Omega} h(x) \cdot f(x) dx$ (for continuous random variables) 



The spread or variability associated with a random variable can be calculated using expected values as well.

:::{.definition}
The ***population variance*** of a random variable $X$ is given by: \

(i) $Var(X) = \sum_{x \in \Omega} (x-\mu)^2 \cdot p(x)$ (for discrete random variables)

(ii) $Var(X) = \int_{x \in \Omega} (x-\mu)^2 \cdot f(x) dx$ (for continuous random variables) 

:::


There is also a shortcut formula for calculating variance: 

:::{.theorem}

$Var(X) = E[X^2] - (E[X])^2$

:::

:::{.definition}

The ***population standard deviation*** of a random variable $X$ is given by: \

$SD(X) = \sigma = \sqrt{Var(X)} = \sqrt{E[X^2]-(E[X])^2}$
:::

:::{.example}
For the Rockies/Cubs four game series example, calculate $E[X]$, $E[X^2]$, and $Var(X)$.
:::

\vfill

\newpage

:::{.definition}
A ***quantile-quantile plot*** (QQ plot) can be used to compare an empirical probability distribution against a theoretical distribution.
:::

:::{.example}
Let's generate two simulated datasets. The first dataset will follow a normal distribution with mean 10 and variance 4 and the second dataset will follow a Poisson distribution with rate 5. We'll use QQ-plots to match each simulation with the correct distribution. 
:::

```{r}
set.seed(2022)
sim1.df <- data.frame(x=rnorm(n = 1000,mean = 10,sd = 2))
sim2.df <- data.frame(x=rpois(n = 1000,lambda = 5))
jitter_width <- 0.2

fig1 <- ggplot(sim1.df, aes(sample = x)) +
  stat_qq(position=position_jitter(width = jitter_width,height = jitter_width)) +
  stat_qq_line() +
  ggtitle("Sim 1 vs. Normal") +
  xlab("Theoretical Quantiles") +
  ylab("Empirical Quantiles")

fig2 <- ggplot(sim2.df, aes(sample = x)) +
  stat_qq(position=position_jitter(width = jitter_width,height = jitter_width)) +
  stat_qq_line() +
  ggtitle("Sim 2 vs. Normal") +
  xlab("Theoretical Quantiles") +
  ylab("Empirical Quantiles")

fig3 <- ggplot(sim1.df, aes(sample = x)) +
  stat_qq(distribution = qpois, dparams = 5,position=position_jitter(width = jitter_width,height = jitter_width)) +
  stat_qq_line(distribution = qpois, dparams = 5) +
  ggtitle("Sim 1 vs. Poisson") +
  xlab("Theoretical Quantiles") +
  ylab("Empirical Quantiles")

fig4 <- ggplot(sim2.df, aes(sample = x)) +
  stat_qq(distribution = qpois, dparams = 5,position=position_jitter(width = jitter_width,height = jitter_width)) +
  stat_qq_line(distribution = qpois, dparams = 5) +
  ggtitle("Sim 2 vs. Poisson") +
  xlab("Theoretical Quantiles") +
  ylab("Empirical Quantiles")
```

\newpage

```{r}
library(gridExtra)
grid.arrange(fig1,fig2,fig3,fig4,ncol=2)
```

What do you conclude from the four QQ plots?


\newpage

## Common Random Variables

There are several families of random variables that show up frequently in applications. Some of these random variables include: Binomial, Geometric, Poisson, Normal 

### Binomial RVs

:::{.definition}
A ***binomial(n,p) random variable*** is a discrete random variable that counts the numbers of "successes" over a fixed number of trials, $n$, with each trial having an equal probability of success, $p$.

$P(X=k) = \binom{n}{k} p^k(1-p)^{n-k} = \frac{n!}{k!\ \cdot\ (n-k)!} p^k(1-p)^{n-k}$, where $0 \leq k \leq n, 0 \leq p \leq 1$

If $X \sim Binomial(n,p)$, then $E[X]=np$ and $Var(X)=np(1-p)$
:::

:::{.example}
The Cubs and Rockies are playing a 4-game series. The Rockies have a 0.65 probability of winning each game, and the Cubs have a 0.35 probability. Assume each game is independent. Solve for the following quantities.
:::

(a) The Cubs win exactly 1 game. \
\
\
\
\vfill

(b) The Rockies win exactly 2 games.\
\
\
\
\vfill


(c) The series ends in a sweep. \
\
\
\
\vfill

(d) The expected number of wins for the Rockies.\
\
\
\
\vfill

(e) The variance and standard deviations of wins for the Rockies.\
\
\
\
\vfill

\newpage

:::{.example}
Complete 10,000 simulations of the four game series between the Rockies and Cubs. For the number of Rockies wins, calculate the sample mean and sample variance and compare these to the population values. Also, plot a histogram of the sample data.
:::

```{r}
set.seed(2022)
rockies_wins <- rbinom(n=10000,size=4,prob=0.65)
mean(rockies_wins)
var(rockies_wins)
rockies_wins_df <- data.frame(Wins=rockies_wins)
rockies_wins_df %>% ggplot(aes(Wins)) + geom_histogram(binwidth = 1,color = "black", fill = "purple")
```

\vfill

\newpage

<!-- #### Binomial Coefficient Symmetry -->

<!-- Playoff series for a certain sports league are played as a best-of-seven series, with one team hosting four games and the opposing team hosing three. An executive for the league wishes to know the number of ways the home and away games can be assigned. (One such combination is A-A-B-B-A-B-A, the format used by the NBA and NHL for their best-of-seven series.) What is the total number of combinations?\ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->

<!-- However, instead of thinking about the number of ways to assign the games to the team that gets four home games, what if we thought about the number of ways to assign games to the team that gets three home games? -->

<!-- That would be $\binom{7}{3}$. We can use the `choose` command in R to find this quantity. -->

<!-- ```{r} -->
<!-- choose(7,3) -->
<!-- ``` -->

<!-- It turns out that this binomial coefficient is also equal to 35. -->

<!-- Theorem: $\binom{n}{k} = \binom{n}{n-k}$ -->

<!-- $\binom{n}{k} = \frac{n!}{k!\ \cdot\ (n-k)!}$ -->

<!-- $\binom{n}{n-k} = \frac{n!}{(n-k)!\ \cdot\ (n-(n-k))!} = \frac{n!}{(n-k)!\ \cdot\ k!} = \binom{n}{k}$ -->

\newpage

### Geometric RVs

:::{.definition}
A ***Geometric(p) random variable*** is a discrete random variable that counts the numbers of trials until a "success" occurs, where the probability of success, $p$, is constant across all trials.

$P(X=k) = p(1-p)^{k-1}$, where $k \geq 1, 0 \leq p \leq 1$

If $X \sim Geometric(p)$, then $E[X]=\frac{1}{p}$ and $Var(X)=\frac{p}{1-p}$
:::


:::{.example}
Suppose the number of shots needed by a hockey team in order to score their first goal, X, is modeled by a Geometric($\frac{1}{10}$) random variable. Use this information to answer the following questions.
:::

(a) What is the probability that it takes exactly 3 shots to score the first goal? \
\
\
\
\vfill

(b) What is the probability that it takes less than 3 shots to score the first goal? \
\
\
\
\vfill

(c) What is the probability that it takes more than 3 shots to score the first goal? \
\
\
\
\vfill

(d) What is the expected number of shots to score the first goal? \
\
\
\
\vfill



\newpage

**Caution:** Some references parameterize the Geometric distribution based on the number of failures before the first success, rather than the trial on which the first success occurs. This changes the PMF, mean, and variance, so be careful.


Let's simulate the number of shot attempts required to score the first goal (Geometric($p=1/10$)) from the previous example.

```{r}
set.seed(2020)
geometric <- rgeom(10000, 1/10)
head(geometric, 20)
```

Some of the values were 0, which could not happen if R was considering the number of the trial on which the first success occurred. You can add 1 to the values given by `R` to arrive at the first success distribution.

```{r}
first_success <- geometric + 1
head(first_success, 20)
(mean_success <- mean(first_success))
```
The mean of this sample of variables is `r mean_success`, which is close to the expected mean of $\frac{1}{p} = 10$.

\newpage

Let's plot the sample distribution of shots required to score a goal from the simulation as well.

```{r}
first_success_df = data.frame(Shots = first_success)
first_success_df %>% ggplot(aes(x=Shots)) + geom_histogram(binwidth = 1)
```

\newpage

### Poisson RVs

:::{.definition}
A ***Poisson($\lambda$) random variable*** is a discrete random variable that counts the numbers of "successes" for a given rate parameter, $\lambda$, for a given interval.

$P(X=k) =  \frac{e^{-\lambda}\lambda^k}{k!}$, where $k \geq 0,$

If $X \sim Poisson(\lambda)$, then $E[X]=\lambda$ and $Var(X)=\lambda$
:::

:::{.example}
During the 2021 Major League Soccer season, the Colorado Rapids scored 51 goals in 34 games on their way to a first-place finish in the Western Conference regular season standings.

The team scored $\frac{51}{34} = 1.5$ goals per game. Let's model the distribution of Rapids goals using a Poisson(1.5) random variable that we'll call Y.
:::

(a) Which is more likely: Y taking on the value 0 or Y taking on the value 2?\
\
\
\

We can calculate these probabilities in R using the `dpois` command.

```{r}
dpois(x=0, lambda=1.5)
dpois(x=2, lambda=1.5)
```

\newpage

We can also plot the PMF of Y to check visually.

```{r}
x <- 0:8
y <- dpois(x, lambda = 1.5)
poisson.pmf <- data.frame(x,y)
ggplot(transform(poisson.pmf), aes(x, y)) + 
  geom_bar(stat="identity") + 
  ggtitle("Probability mass function of Poisson(1.5) random variable") +
  xlab("Value") +
  ylab("Probability")
```

\newpage

Let's check whether using a Poisson distribution was appropriate by comparing it to the actual 2021 Colorado Rapids match results.

```{r}
# Data: https://www.espn.com/soccer/team/results/_/id/184/season/2021

library("kableExtra")
rapids21 <- 
  c(0,1,1,3,3,1,3,2,1,1,2,1,2,0,1,0,3,2,2,1,1,1,2,1,0,3,0,3,1,1,2,0,1,5)

goals <- c(0:4, "5+")
actual_frequency <- c(6, 14, 7, 6, 0, 1)
actual_proportion <- actual_frequency / sum(actual_frequency)
expected_proportion <- c(dpois(0:4, lambda=1.5), 
                         ppois(4, lambda=1.5, lower.tail=FALSE))
expected_frequency <- round(expected_proportion * 34, 1)

rapids.data <- data.frame(goals, actual_frequency, actual_proportion, 
                          expected_frequency, expected_proportion)
names(rapids.data) = c("Goals","Actual Frequency","Actual Proportion","Expected Frequency","Expected Proportion")

rapids.data %>% kt()
```

(b) What differences do you notice between the actual results and the expected values based on the Poisson random variable? \
\
\
\
\vfill

<!-- There were fewer games in which the Rapids scored 4 or more goals than the model would indicate, yet the Rapids were shut out less often than the model would indicate. -->

(c) Even if the true population distribution of 2021 Rapids goals was truly a Poisson(1.5) random variable, why might the actual distribution of their goals differ from the probability mass function? \
\
\
\
\vfill

<!-- 34 is a relatively small sample size; random variables may not coincide with their expected values for finite sample sizes. -->

\newpage

(d) Use a QQ plot to compare the probability distribution of the 2021 Rapids goals to a Poisson distribution. \

```{r}
rapids21 %>% 
  as.data.frame %>% 
  ggplot(aes(sample = rapids21)) +
  stat_qq(distribution = qpois,
          dparams = 1.5,position=position_jitter(width = 0.1,height = 0.1)) +
  stat_qq_line(distribution = qpois, dparams = 1.5) +
  ggtitle("QQ-plot for Rapids 2021 Goals vs. Poisson Distribution") +
  xlab("Theoretical Quantiles") +
  ylab("Empirical Quantiles")
```

(e) What are the advantages of using the Poisson distribution to model Major League soccer goals? What are the disadvantages? \
\
\
\

<!-- Poisson random variables can take on the natural numbers (including zero), which aligns with the number of goals that can be scored in a match. One disadvantage is that it is possible for a Poisson to take on values that are not realistic for the situation, such as double-digit integers or higher. Only one game in MLS history has had a team score more than seven goals in a game. However, when $\lambda$ is small (such as 1.5), these extreme values are relatively unlikely. -->


\newpage

:::{.example}
In 1997-1998 with the Los Angeles Lakers, Shaquille O'Neal attempted an average of 11.35 free throws per game with a standard deviation of 4.04. Is it appropriate to model Shaq's per game free throw attempts as a Poisson(11.35) random variable? 
:::

(a) Plot the data.
```{r, message=F}
shaq9798 <- read_csv("data/shaq9798.csv")
shaq9798 %>% ggplot(aes(x=FTA))  + 
  geom_bar(color = "yellow", fill = "purple") +
  ggtitle("Per Game FT Attempt Totals by Shaq in 1997-1998") +
  xlim(0,25)
```

\newpage

(b) Plot the PMF of a Poisson(11.35) random variable.
```{r}
x <- 0:25
y <- dpois(x, lambda = 11.35)
poisson.pmf <- data.frame(x,y)
ggplot(poisson.pmf, aes(x, y)) + 
  geom_bar(stat="identity") + 
  ggtitle("Probability mass function of Poisson(11.35) random variable") +
  xlab("Value") +
  ylab("Probability")
```

(c) What similarities and what differences do you notice?\
\
\
\
\vfill

\newpage

(d) Calculate the variance of the two distributions and compare them.

```{r}
var(shaq9798$FTA)
# Var(Poisson(11.35)) = 11.35
```


(e) Calculate the probability that Shaq had 20 or more free throws and compare it to $P(Poisson(11.35) \geq 20)$

```{r}
shaqFTA <- shaq9798$FTA
shaq20 <- sum(shaqFTA >= 20)/length(shaqFTA); shaq20
poisson20 <- ppois(20, lambda=11.35, lower.tail=FALSE); poisson20
```

\newpage

(f) Create a QQ-plot of Shaq's per game free throw attempts and a Poisson distribution.

```{r}
shaq9798 %>% 
  ggplot(aes(sample = FTA)) +
  stat_qq(distribution = qpois,dparams = 11.35,position=position_jitter(width=0.2,height=0.2)) +
  stat_qq_line(distribution = qpois, dparams = 11.35) +
  ggtitle("QQ-plot for Shaq's Free Throw Attempts vs. Poisson Distribution") +
  xlab("Theoretical Quantiles") +
  ylab("Empirical Quantiles")
```
 
(g) Is the Poisson distribution appropriate to model Shaq's FTA per game? Explain. \
\
\
\

\newpage

### Negative Binomial RVs

:::{.definition}
A ***Negative Binomial($r$,$p$) random variable*** is a discrete random variable that counts the numbers of "successes" for given parameters, $r$ and $p$.

$P(X=k) =  {k+r-1 \choose k}(1-p)^rp^k$, where $k = 0, 1, 2, ...$

If $X \sim NB(r,p)$, then $E[X]=\frac{rp}{1-p}$ and $Var(X)=\frac{rp}{(1-p)^2}$
:::

The Negative Binomial distribution is often used to model count data that is "overdispersed". A property of the Poisson distribution is that the mean and variance are equal. If you are analyzing count data such that the variance is much greater than the mean (i.e., overdispersed), then the Negative Binomial distribution may be an appropriate substitute.

Given sample count data, we can estimate appropriate parameters for a Negative Binomial in many ways. One such way is to use the "method of moments" estimator.

These estimators are given by:

$\hat{p} = \frac{s^2-\bar{x}}{s^2}$ and $\hat{r} = \frac{\bar{x}^2}{s^2-\bar{x}}$

:::{.example}
Using Shaq's 1997-1998 data, model his per game free throw attempts as a Negative Binomial random variable.
:::

(a) Find an appropriate choice of parameters, $r$ and $p$.

```{r}
shaq.mean <- mean(shaqFTA)
shaq.var <- var(shaqFTA)
rhat <- shaq.mean^2/(shaq.var-shaq.mean)
phat <- (shaq.var-shaq.mean)/shaq.var
c(rhat,phat)
```


(b) Plot the Negative Binomial distribution. Note that R uses an alternative parameterization for $p$. Use $prob = 1-p$.

```{r}
x <- 0:25
y <- dnbinom(x,size=rhat,prob=1-phat)
geom.pmf <- data.frame(x,y)
ggplot(geom.pmf, aes(x, y)) + 
  geom_bar(stat="identity") + 
  labs(x="Value", y="Frequency", title="Probability mass function of NB(r=25.852,p=0.305) random variable")
```

(c) Calculate the mean and variance of the Negative Binomial and Shaq's dataset.

```{r}
shaq.mean <- mean(shaqFTA)
shaq.var <- var(shaqFTA)
NB.mean <- (rhat*phat)/(1-phat)
NB.var <- (rhat*phat)/(1-phat)^2
c(shaq.mean,shaq.var)
c(NB.mean,NB.var)
```

(d) Calculate the probability that Shaq had 20 or more free throws and compare it to $P(NB(r=25.852,p=0.305) \geq 20)$

```{r}
shaq20 <- sum(shaqFTA >= 20)/length(shaqFTA); shaq20
nb20 <- pnbinom(20,size=rhat,prob=1-phat,lower.tail=FALSE); nb20
```

\newpage

(e) Create a QQ-plot of Shaq's per game free throw attempts and a Negative Binomial distribution.

```{r}
shaq9798 %>% 
  ggplot(aes(sample = FTA)) +
  stat_qq(distribution = qnbinom,dparams = list(size=25.852,mu=11.35),position=position_jitter(width=0.2,height=0.2)) +
  stat_qq_line(distribution = qnbinom, dparams = list(size=25.852,mu=11.35)) +
  ggtitle("QQ-plot for Shaq's Free Throw Attempts vs. Negative Binomial Distribution") +
  xlab("Theoretical Quantiles") +
  ylab("Empirical Quantiles")
```


(f) Is the Negative Binomial distribution appropriate to model Shaq's FTA per game? How does it compare to using the Poisson distribution? Explain. \
\
\
\

\newpage

### Normal RVs

:::{.definition}
A ***Normal($\mu$,$\sigma^2$) random variable*** is a continuous random variable that is bell-shaped with mean $\mu$ and variance $\sigma^2$.

To calculate probabilities under the normal curve, you need either to integrate, use a table, or a computer.

Note that a normal random variable can be standardized by using: $z = \frac{x-\mu}{\sigma}$
:::

:::{.theorem}
For a normal($\mu$,$\sigma^2$) random variable, we have the following approximations: \
- About 68\% of the data falls within one standard deviation of the mean (i.e., $\mu \pm \sigma$) \
- About 95\% of the data falls within two standard deviations of the mean (i.e., $\mu \pm 2\sigma$) \
- About 99.7\% of the data falls within three standard deviations of the mean (i.e., $\mu \pm 3\sigma$)
:::

:::{.example}
The skills (or tools) of a baseball player are often rated on a scale of 20-80, where 50 is an average grade, 20 is the lowest grade, and 80 is the highest grade. The distribution of tool grades is approximately normally distributed ($\mu=50, \sigma =10$).

See [https://blogs.fangraphs.com/scouting-explained-the-20-80-scouting-scale/](https://blogs.fangraphs.com/scouting-explained-the-20-80-scouting-scale/) for more details. Calculate the following probabilities.
:::

(a) Former Rockie Nolan Arenado has been graded to have game power of 70. Game power estimates a player's ability to hit home runs. Approximately what percentage of baseball players have equal or greater game power than Arenado? \
\
\
\
\vfill

(b) Mike Trout has been graded to have raw power of 55. Raw power estimates a player's ability to hit baseballs hard (i.e., hard hit rate). Approximately what percentage of baseball players have equal or less raw power than Trout? \
\
\
\
\vfill

\newpage

(c) Suppose a Rockies prospect is said to be in the top 10\% of all baseball players in terms of their speed. What approximate speed grade would correspond to the player? \
\
\
\
\vfill
x

(d) Suppose a Rockies prospect is said to be in the bottom 20\% of all baseball players in terms of their hit ability. What approximate hit grade would correspond to the player? \
\
\
\
\vfill


(e) Between what two grades do approximately 95\% of all players lie for a given tool? \
\
\
\
\vfill

Let's check our answers:
```{r}
a <- 1-pnorm(q=70,mean=50,sd=10); a
b <- pnorm(q=55,mean=50,sd=10); b
c <- qnorm(0.1,mean=50,sd=10,lower.tail = F); c
d <- qnorm(0.2,mean=50,sd=10,lower.tail = T); d
e <- pnorm(q=70,mean=50,sd=10) - pnorm(q=30,mean=50,sd=10); e
```

\newpage

<!-- :::{.example} -->
<!-- Player X has a projected mean WAR of 3 with standard deviation of 2 and player Y has a projected mean WAR of 1.5 with a standard deviation of 3. Assume projected WAR is normally distributed. What is the probability that Player X outperforms Player Y? -->

<!-- Link to WAR explanation: https://www.mlb.com/glossary/advanced-stats/wins-above-replacement -->
<!-- ::: -->

<!-- We want $P(X>Y)$ or $P(X-Y>0)$ \ -->

<!-- \vfill -->

<!-- ```{r} -->
<!-- # Calculate probability Z<=0 -->
<!-- p <- pnorm(0,1.5,sqrt(5),lower.tail = F); p -->
<!-- ``` -->

<!-- \vfill -->

\newpage

## Win Probability Models

The probability an NFL team will win can be modeled as a normal random variable using the Vegas line. This method is outlined by Wayne Winston in *Mathletics* building on previous research by Hal Stern.

### Estimating Pregame Win Probability

Winston estimates that the final margin of victory is approximately a normal random variable with a mean of the Vegas line and a standard deviation between 13-14. Winston and Stern estimated the standard deviation to be **13.86** based on data from the 1981, 1983, and 1984 NFL seasons.

This estimated standard deviation has since been updated to be **13.45** based on data from 1978 -- 2012 NFL seasons.

Reference: https://www.pro-football-reference.com/about/win_prob.htm

:::{.example}
Sketch the distribution of the final margin of victory for an NFL team that is favored by 7 points. Shade the area for a win in regulation (for the favorite) and a tie in regulation.
:::

```{r,fig.height=2}
library(ggplot2)
# library(gridExtra)


final_margin_full <- ggplot(data.frame(x = c(-21, 42)), aes(x = x)) +
        stat_function(fun = dnorm, args = list(mean = 7, sd = 13.45)) + 
  xlab("Final Margin (for favorite)") + ggtitle("Score Distribution")
final_margin_full
```

\newpage

:::{.example}
In Super Bowl 50, the Denver Broncos were 5.5 point underdogs (+5.5) against the Carolina Panthers. Estimate the pregame win probability for the Denver Broncos. \
:::

```{r,fig.height=2, echo = F}

x_min <- 5.5 - 42
x_max <- 5.5 + 42

x_min1 <- 5.5 - 6.5
x_max1 <- 5.5 + 6.5

final_margin_full <- ggplot(data.frame(x = c(x_min,x_max)), aes(x = x)) +
        stat_function(fun = dnorm, args = list(mean = 5.5, sd = 13.45)) + 
  xlab("Final Margin (for favorite)") + ggtitle("Score Distribution")
final_margin_full
```

\vfill

```{r pregame_wp}
fav_line <- 5.5
full_sd <- 13.45

win_prob <- pnorm(q = 0.5, mean = fav_line, sd = full_sd, lower.tail = FALSE)

tie_prob <- pnorm(q = 0.5, mean = fav_line, sd = full_sd, lower.tail = TRUE) -
            pnorm(q = -0.5, mean = fav_line, sd = full_sd, lower.tail = TRUE)

# win probability for favorite (panthers)
win_prob <- win_prob + 1/2 * tie_prob

# win probability for underdog (broncos)
(broncos_win_prob <- 1 - win_prob)
```

\newpage

### Estimating In-Game Win Probability

Winston proposed an updated method to calculate in-game win probabilities based on the time remaining in the game, current score margin, and Vegas line.

In this proposed method, the pregame mean (Vegas line) and pregame variance (13.45) are scaled down linearly as a function of time remaining in the game.

For instance, since there are four 15-minute quarters (total regulation game time is 60 minutes), then the in-game mean and standard deviation are calculated as follows.

$\mu_{updated} = Line \cdot \frac{\text{time remaining}}{60}$

$\sigma_{updated} = \frac{13.45}{\sqrt{60/\text{time remaining}}}$

**Note:** This model assumes perfectly neutral possession, down, distance, and field-position conditions.

Reference: https://www.pro-football-reference.com/about/win_prob.htm

:::{.example}
In Super Bowl 50, the Denver Broncos led the Carolina Panthers 10-0 after the first quarter. The Broncos were 5.5 point underdogs at the start of the game. Use Winston's method to estimate the probability of a Broncos win after one quarter. \
:::

```{r in_game_wp}
fav_margin <- -10
fav_line <- 5.5
full_sd <- 13.45
full_min <- 60
new_min <- 45

new_mean <- fav_line * new_min/full_min
new_sd <- full_sd / sqrt(full_min/new_min)

win_prob <- pnorm(q = 0.5 - fav_margin, mean = new_mean, 
                  sd = new_sd, lower.tail = FALSE)

tie_prob <- pnorm(q = 0.5 - fav_margin, mean = new_mean, 
                  sd = new_sd, lower.tail = TRUE) -
            pnorm(q = -0.5 - fav_margin, mean = new_mean, 
                  sd = new_sd, lower.tail = TRUE)

# win probability for favorite (panthers)
win_prob <- win_prob + 1/2 * tie_prob

# win probability for underdog (broncos)
(broncos_win_prob <- 1 - win_prob)
```

\newpage

The win probability models outlined above use only the Vegas line, the current score, and the time remaining in the game. More accurate estimation models will also consider possession, down, distance, and field-position conditions. Pro Football Reference offers an in-game win probability model with these additional factors.

Reference: https://www.pro-football-reference.com/boxscores/win_prob.cgi

:::{.example}
The Broncos led the Panthers 10-0 at the end of the first quarter. The Panthers had 2nd and 11 at their own 46 at the beginning of the second quarter. Calculate the Broncos win probability.

Reference: https://www.pro-football-reference.com/boxscores/201602070den.htm
:::

![Pro Football Reference Win Probability Calculator Inputs](images/wp1){width=80%}

![Pro Football Reference Win Probability Calculator Output](images/wp2){width=50%}

**Class Reading:** Chapters 6--7 in the *Hidden Game of Football* by Carroll, Palmer, Thorn
