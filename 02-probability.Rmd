# Probability

## Chapter Preview {-}
Probability is the study of randomness. In this chapter, we will define probability, learn rules of probability, and apply these rules to sports data.

## Definitions

:::{.definition}
An ***experiment*** is any activity or process whose outcome is subject to uncertainty.
:::


:::{.definition}
The ***sample space*** of an experiment, denoted by $\Omega$ or $\mathcal{S}$, is the set of all possible outcomes of that experiment.
::: 


:::{.definition}
An ***event*** is any collection (subset) of outcomes contained in the sample space, $\Omega$.
:::

:::{.example}
Give an example of a discrete random variable in a sports context.
:::
\
\
\
\
\

:::{.example}

Give an example of a continuous random variable in a sports context.
:::
\ 
\
\
\
\

\newpage

## Set Theory 

For the following examples, suppose that we are interested in the batting outcomes of a plate appearance in softball.

Let $A$ be the event that the batter gets walked, let $B$ be the event that the batter gets a hit, let $C$ be the event that the batter strikes out, and let $D$ be the event that the batter makes it to first base at the end of their at bat.

We will define a handful of set operations to help us when we begin calculating the probability of different events occurring. 

:::{.definition} 
The ***compliment*** of an event $A$, denoted by $A^c$ or $A'$, is the set of all outcomes in $\Omega$ that are not contained in $A$.
:::

:::{.example}
Draw a Venn diagram illustrating $A^c$ and describe the event.
:::

\
\
\
\
\

:::{.definition}
The ***union*** of two events $A$ and $B$, denoted by $A \cup B$ and read "$A$ or $B$", is the event consisting of all outcomes that are either in $A$ or $B$ or in both.
:::

:::{.example}
Draw a Venn diagram illustrating $A \cup D$ and describe the event.
:::

\
\
\
\
\

:::{.definition}
The ***intersection*** of two events $A$ and $B$, denoted by $A \cap B$ and read "$A$ and $B$", is the event consisting of all outcomes that are in both $A$ and $B$.
:::

:::{.example}
Draw a Venn diagram illustrating $A \cap D$ and describe the event.
:::

\
\
\
\
\

:::{.definition}
The ***difference*** of two events $A$ and $B$, denoted by $A \mathbin{/} B$ and read "difference of $A$ and $B$", is
the event consisting of all outcomes that are in $A$ but not in $B$.
:::

:::{.example}
Draw a Venn diagram illustrating $D \mathbin{/} A$ and describe the event.
:::

\
\
\
\
\



:::{.definition}
Two events $A$ and $B$ are said to be ***disjoint*** (or ***mutually exclusive***) if $A \cap B = \emptyset$
:::

:::{.example}
Are the events $A$ and $B$ disjoint? How about $A$ and $D$?
:::

\
\
\
\
\

\newpage




## Axioms, Properties, and Laws

There are some basic assumptions of "axioms" which are the foundation of the theory of probability. Andrey Kolmogorov first described these axioms in 1933.

### Axioms of Probability
1. $P(A) \geq 0$, for any event $A$ \
2. $P(\Omega) = 1$ \
3. If $A_1, A_2, A_3, \ldots$ is a collection of disjoint events, then: \
$P(\cup_{i=1}^{\infty} A_i) = P(A_1 \cup A_2 \cup \ldots ) = \sum_{i=1}^{\infty} P(A_i)$

Note that all probabilities are between 0 and 1, that is, for any event $A$, $0 \leq P(A) \leq 1$.

We can convert to percentages by multiplying probabilities by 100, however, this is a set that is only done after all calculations have been completed.

### Properties of Probability

- $P(\emptyset) = 0$ \

- $P(A^c) = 1 - P(A)$ 

- $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

- $P(A \cup B \cup C) = \\ P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$

- $P([A \cup B]^c) = P(A^c \cap B^c)$

- $P([A \cap B]^c) = P(A^c \cup B^c)$

\newpage

:::{.example}
In 2001, Barry Bonds broke the single season home run record with 73 home runs. In this season, he had 664 plate appearances (476 at-bats), 156 hits, 177 walks, 9 hit by pitches, and 2 sacrifice flies. Use this information to answer the following questions.
:::

(a) Plate appearances that result in a walk, hit by pitch, or sacrifice fly are not counted towards a player's at-bats. Confirm that Bonds had 476 official at-bats. \
\
\
\
\vfill

(b) Suppose an at-bat is chosen at random. What is the probability that Bonds got a hit? (This is his batting average.) \
\
\
\
\vfill

*For the following examples, assume that one of Bonds' plate appearances is chosen at random.* \

(c) What is the probability that Bonds reached base via a hit, walk, or hit by pitch? (This is his on-base average/percentage.) \
\
\
\
\vfill

(d) What is the probability that Bonds did not reach base? \
\
\
\
\vfill

(e) Calculate $P\left(HBP \cup Walk\right)$ \
\
\
\
\vfill

(f) Calculate $P\left(HBP^c \cap Walk^c\right)$ \
\
\
\
\vfill

\newpage


### Laws of Probability

:::{.definition}
Let $A$ and $B$ be two events such that $P(B)>0$. Then the ***conditional probability*** of $A$ given $B$, written $P(A|B)$, is given by:
$P(A|B) = \frac{P(A \cap B)}{P(B)}$
:::

:::{ .example}
In 1998, Sammy Sosa hit 66 home runs in 722 plate appearances, the third highest single season homework total ever. During June 1998, Sosa 20 home runs in 121 plate appearances. Suppose a randomly selected plate appearance is selected. Calculate the following probabilities.
:::

(a) $P(HR)$ \
\
\
\
\vfill


(b) $P(HR \cap June)$ \
\
\
\
\vfill

(c) $P(June)$ \
\
\
\
\vfill

(d) $P(HR|June)$ \
\
\
\
\vfill

(e) $P(June|HR)$ \
\
\
\
\vfill

(f) $P(HR|June^c)$ \
\
\
\
\vfill


:::{.theorem name="Multiplication Rule"}
For any two events $A$ and $B$, $P(A \cap B) = P(B|A) \cdot P(A)$.
:::

:::{.example}
Calculate the probability that a randomly selected plate appearance from Sosa's 1998 season is a home run in June.
:::

\
\
\
\vfill

\newpage



:::{.definition}
Events $A_1, A_2, \ldots, A_n$ are said to form a ***partition*** of a sample space $\Omega$ if both: \
(i) $A_i \cap A_j = \emptyset$ ($i \neq j$) \
(ii) $\cup_{i=1}^n A_i = \Omega$ \
:::


:::{.theorem name="Law of Total Probability"}
Suppose events $A_1, A_2, \ldots, A_n$ form a partition of $\Omega$, then:
$P(B) = P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + \ldots P(B|A_n)P(A_n)$
:::

:::{.example}
What is one possible way to partition Sosa's plate appearances in 1998?
:::

\
\
\



:::{.theorem name="Bayes Theorem: simple version"}
Suppose events $B$ and $C$ form a partition of $\Omega$, then:
$P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|C)P(C)}$
:::

:::{.theorem name="Bayes Theorem"}
Suppose events $B_1, B_2, \ldots, B_n$ form a partition of $\Omega$, then:
$P(B_k|A) = \frac{P(B_k \cap A)}{P(A)} = \frac{P(A|B_k)P(B_k)}{P(A|B_1)P(B_1)+P(A|B_2)P(B_2) + \ldots + P(A|B_n)P(B_n)}$
:::

:::{.example}
Given that Sosa hit a home run in 1998, what is the probability that he hit it in June?
:::

\
\
\


:::{.example}
Over the course of a season, a hockey player scored a goal 30% of the time during a home game, and 18%. Assume all games are either home or away. Use this information to answer the following questions.
:::

(a) What is the probability the player scored a goal in any game if there were an equal number of home and away games? \
\
\
\

(b) What is the probability the player scored a goal in any game if there were twice as many home games as away games? \
\
\
\

(c) What is the probability the player scored a goal in any game if the ratio of home games to away games is 2:3? \
\
\
\


\newpage

## Combinatorics

Combinatorics is the mathematical study of counting, particularly with respect to permutations and combinations.

:::{.definition}
The ***factorial function ($n!$)*** is defined for all positive integers by: $n! = n \cdot (n-1) \cdot \ldots 2 \cdot 1$
:::

Note that $0! \equiv 1$ and $1! \equiv 1$.

:::{.example}
A baseball/softball batting lineup has nine ordered players. Suppose the manager has selected the nine players to bat. How many different batting orders are there possible?
:::

\
\
\

:::{.definition}
An ordered subset is called a ***permutation***. The number of permutations of size $k$ that can be formed from the $n$ elements in a set is given by: $P_{n,k} = \frac{n!}{(n-k)!}$
:::

:::{.example}
In MLB, each team has a 26-person roster, of which about 13 are hitters. Assuming one of the batters is the designated hitter, how many different batting lineups of 9 players can a manager create?
:::

\
\
\

:::{.definition}
An unordered subset is called a ***combination***. The number of combinations of size $k$ that can be formed from the $n$ elements in a set is given by: $C_{n,k} = {n \choose k} = \frac{n!}{k! \cdot (n-k)!}$
:::

:::{.example}
Suppose a manager has 13 hitters to choose from to fill out a starting lineup of 9 players. Ignoring positions, how many different ways can the manager pick a 9-person lineup from 13 possible hitters?
:::

:::{.theorem name="Product Rule for Ordered Pairs"}
If the first element of an ordered pair can be selected in $n_1$ ways and for each of the these $n_1$ ways the second element of the pair can be selected in $n_2$ ways, then the number of pairs is $n_1 \cdot n_2$.
:::

\
\
\
\

:::{.theorem name="Generalized Product Rule"}
Suppose a set consists of $k$ elements (k-tuples) and that there are $n_1$ possible choices for the first element, $n_2$ possible choices for the second element, ... , and $n_k$ possible choices for the $k^\text{th}$ element, then there are $n_1 \cdot n_2 \cdot \ldots \cdot n_k$ possible k-tuples.
:::

:::{.example}
A baseball manager selects nine hitters and one pitcher for a starting lineup. Suppose they can choose from 13 hitters and 13 hitters. How many different possible lineups can the manager choose from?
:::

\
\
\

:::{.example}
In softball, rules sometimes allow for a "designated player". This player hits for any position player or pitcher but isn't required to play defense. There is some flexibility to allow the designated player to temporarily play a position during the game however. For more on the designated player rule in softball, visit: https://cdn2.sportngin.com/attachments/document/c780-2407390/DPFlexRuleExplained.pdf

In 2022, the CSU softball team had 24 players. Three of these players were only pitchers and two of these players were both pitchers and fielders. How many different lineups of 10 (8 non-pitching position players, 1 pitcher, 1 designated player) are possible for CSU?
:::

\
\
\
\

\newpage

## Odds and Gambling

### Sports Betting in USA

In 2018, the United States Supreme Court overturned a 1992 federal law that banned commercial sports betting in most states. For more about this ruling, see this New York Times article: https://www.nytimes.com/2018/05/14/us/politics/supreme-court-sports-betting-new-jersey.html

In Colorado, you must be at least 21 years old to gamble including betting on sports games.

### Gambling Addiction

Gambling addiction, compulsive gambling, or gambling disorder is a serious impulse-control disorder. Here is a link to the Mayo Clinic's discussion of compulsive gambling: https://www.mayoclinic.org/diseases-conditions/compulsive-gambling/symptoms-causes/syc-20355178

Gambling is ***not*** encouraged and can lead to numerous problems. If you choose to gamble on sports, don't bet beyond your means and seek help if you worry that you may be suffering from gambling addiction.

### Odds Definition

In this chapter so far, we have quantified uncertainty using probabilities (numbers between 0 and 1) and percentages (numbers between 0 and 100). We can also quantify uncertainty using **odds**.

:::{.definition}
The ***odds*** (in favor) of an outcome $A$ is the probability of $A$ divided by the probability of $A^C$.

\[Odds = \frac{p}{1-p}\]
:::

**Note:** We may also be given the **odds against** a particular outcome. In this case, we have $Odds Against = \frac{1-p}{p}$.\

**Some Common Odds:**

```{r, echo=F}
a1 <- c(0:5,10,100)
a2 <- c(rep(1,8))
b1 <- c(rep(1,8))
b2 <- c(0:5,10,100)
p1 <- a1/(a1+b1)
p2 <- a2/(a2+b2)
q1 <- b1/(a1+b1)
q2 <- b2/(a2+b2)
odds1 <- c("0:1","1:1","2:1","3:1","4:1","5:1","10:1","100:1")
odds2 <- c("1:0","1:1","1:2","1:3","1:4","1:5","1:10","1:100")
odds_df <- data.frame(Odds = odds1, p = p1, q = q1, Odds = odds2, p = p2, q = q2)
names(odds_df) = c("Odds","p","q","Odds","p","q")
odds_df %>% kt()
```

In the following table, the odds of an outcome along with the probability of the outcome, *p*, and the probability of the outcome's complement, *q=1-p* are given.

:::{.example}
For the following examples, convert between probability and odds.
:::

(a) Suppose that the probability that the Cubs win the 2025 World Series is 0.03. What are the odds in favor? \

\
\
\

(b) Suppose that the odds that the Rockies win the World Series in 2025 is 100:1. What is the probability in favor? \

\
\
\

(c) Suppose there is a 1\% chance that CSU wins a national championship in any sport in the next decade. What are the probability and the odds against CSU winning a national championship in the next decade?\

\
\
\

:::{.example}
Suppose that there are three finalists in an Olympic competition. It is estimated that Athlete A has a 50\% chance of winning, Athlete B has a 40\% chance of winning, and Player C has a 10\% chance of winning. Calculate the odds against each of the athletes winning the competition.
:::

\
\
\
\
\

### Gambling Odds

Here's a helpful resource on calculating gambling odds:
https://www.actionnetwork.com/education/decimal-odds

Gambling odds are a bit different that odds in a probability context. Gambling odds tell the amount that the bookmaker will pay out for a winning bet. For example, if a bookmaker is offering "10:1" that the Rockies win the next World Series, the bookmaker will pay out 10x the original wager plus the original wager if the Rockies win the World Series and the bookmaker will keep the original bet if the Rockies fail to win the World Series.

:::{.definition}
***Fractional odds*** are common in horse racing and quote the net total that will be paid out to the bettor, should he or she win, relative to the stake. The numerator and denominator of fractional odds are always positive integers. For example, a \$100 wager on a "5:1" bet would result in a payout of $5 \cdot \$100 + \$100 = \$600$ if the wager is correct and a payout of \$0 if the wager is incorrect.
:::

:::{.definition}
***Decimal Odds*** represent the multiplier of a winning bet and is calculated by taking the inverse of the implied probability. For instance, if you bet \$100 with decimal odds of 1.8, your payout is \$180 (\$100 wager + \$80 winnings).

\[Decimal \, Odds = [\tilde{p}]^{-1}\]
:::

:::{.definition}
A ***moneyline bet*** are common bets in American sports. When the moneyline is positive, the figure tells what the winning payout would be on a \$100 bet. When the moneyline is negative, the figure tells what wager is required for a winning payout of \$100.
:::

:::{.example}
For the following scenarios, assume a wager of \$100. Calculate the payout for a winning bet.
:::

(a) Fractional odds of 4/1 \
\
\
\

(b) Moneyline +400 \
\
\
\

(c) Moneyline -300 \
\
\
\

(d) Decimal Odds of 1.5 \
\
\
\

**Note:** A *moneyline wager* refers to the odds of a straight-up outcome on a game without consideration of a point spread. Typically, favorites will have negative moneyline odds and underdogs will have a positive moneyline odds.

:::{.definition}
The ***implied probability***, $\tilde{p}$, of a wager is the probability that corresponds to the odds of the wager.

\[\tilde{p} = \frac{Odds}{Odds+1}\]

If we are dealing with moneyline bets, we have the following:

If ML is negative:
\[\tilde{p} = \frac{-ML_{NEG}}{-ML_{NEG}+100}\]

If ML is positive:
\[\tilde{p} = \frac{100}{ML_{POS}+100}\]
:::

:::{.example}
Calculate the implied probabilities for the following examples.
:::

(a) Fractional odds of 4/1 \
\
\
\

(b) Moneyline -125 \
\
\
\

(c) Moneyline +125 \
\
\
\

(d) Decimal Odds of 1.5 \
\
\
\

:::{.example}
Suppose that for an upcoming NFL game, a moneyline wager on the Broncos is -105 and a moneyline wager on the Raiders is -120.
:::

(a) Calculate the implied probabilities of the two possible outcomes. (Note that sometime ties are an optional wager. Other times, ties will result in a push, that is, no winner.) \
\
\
\

(b) What is the sum of the implied probabilities? \
\
\
\

(c) Does the result from (b) violate the axioms of probability? If so, why? \
\
\
\


**Note:** The goal of bookmaking is to make money, so there will be almost certainly be a house advantage. This means that the sum of the implied probabilities will be greater than 1.

:::{.definition}
The ***house advantage*** (or ***hold percentage***) is the percentage of money that sportsbooks keep for every dollar earned.

\[House \, Advantage = 100 \cdot \sum_{i=1}^n \tilde{p}_i - 100\]
:::


:::{.example}
Suppose that a bookmaker is offering a moneyline wager on the Broncos at -105 and a moneyline wager on the Raiders at -120. Calculate the bookmakers expected winnings based on the following amounts of total bets.
:::

(a) \$500 wagered on the Broncos, \$500 wagered on the Raiders \
\
\
\

(b) \$250 wagered on the Broncos, \$750 wagered on the Raiders \
\
\
\

(c) \$750 wagered on the Broncos, \$250 wagered on the Raiders \
\
\
\

(d) \$0 wagered on the Broncos, \$1000 wagered on the Raiders \
\
\
\

(e) \$1000 wagered on the Broncos, \$0 wagered on the Raiders \
\
\
\

(f) What is the house advantage? \
\
\
\

\newpage

:::{.example}
A ***point spread bet*** is a bet based on the projected margin of victory that can result in a win, loss, or push.

For example, if a sportsbook offers Rockies -1.5 against the Cubs and you bet on the Rockies, if the Rockies win by 2 or more runs, you win and if the Rockies win by 1 run or lose, then you lose.
:::

:::{.example}
Suppose a sportsbook offers Broncos +14 against the Raiders at -110 (they also offer Raiders -14 at -110) and you have \$20 to wager. Calculate the payout for the following scenarios.
:::

(a) You bet \$20 on the Broncos and the final score is Broncos 21 Raiders 20. What is your payout? \
\
\
\

(b) You bet \$20 on the Raiders and the final score is Broncos 21 Raiders 20. What is your payout? \
\
\
\

(c) You bet \$10 on the Broncos and \$10 on the Raiders and the final score is Broncos 21 Raiders 20. What is your payout? \
\
\
\

:::{.example}
A ***parlay bet*** is a combination of multiple bets where the winnings from a winning bet are placed on other bets. All bets must be win for the parlay bet to pay out.
:::

:::{.example}
Suppose that CSU is a three point favorite (-3) against CU in a women's basketball game and the over/under on total number of points is 97. Assume that the price of bets is -110.
:::

(a) Suppose you place two bets, CSU -3 and Over 97, and the outcome of the game is CSU 60 CU 40. What is your payout? \
\
\
\

(b) Suppose you place a parlay bet on CSU -3 and Over 97 and the outcome of the game is CSU 60 CU 40. What is your payout? \
\
\
\

(c) Suppose you place two bets, CSU -3 and Over 97, and the outcome of the game is CSU 44 CU 40. What is your payout? \
\
\
\

(d) Suppose you place a parlay bet on CSU -3 and Over 97 and the outcome of the game is CSU 44 CU 40. What is your payout? \
\
\
\




*Reference:* \
https://www.wikihow.com/Calculate-Odds




\newpage

## Random Variables

:::{.definition}
Let $\Omega$ be the sample space of an experiment. A ***random variable*** is a rule that associates a number with each outcome in $\Omega$. In other words, a random variable is a function whose domain is $\Omega$ and whose range is the set of real numbers.
:::

Random variables are be broken down into subcategories:\
1. ***Discrete random variables*** - random variables which have a sample space that is finite or countably infinite.\
2. ***Continuous random variables*** - random variables which have a sample space that is uncountably infinite (such as an interval of real numbers)

***Discrete*** and ***Continuous*** random variables use similar yet slightly different mathematical tools. Discrete random variables involve working with "sums" and continuous random variables involve working with "integrals".

:::{.example}
$$ $$
:::

\
\
\

:::{.example}
$$ $$
:::

\
\
\

:::{.definition}
A ***probability distribution*** is a function that gives probabilities of different possible outcomes for a given experiment.
:::

The probability distribution for a discrete random variable, $p(x)$, is called a ***probability mass function (pmf)***.

The probability distribution for a continuous random variable, $f(x)$, is called a ***probability density function (pdf)***.

:::{.example}
Suppose the Colorado Rockies are playing a four game series against the Chicago Cubs and that the Rockies have a 65\% chance of winning an individual game. Further, assume that the games are independent. The following PMF describes the outcomes (number of Rockies wins) and their probabilities.

```{r, echo=F,message=F}
library(tidyverse)
library(kableExtra)
x <- 0:4
px <- c(0.015, 0.111, 0.311, 0.384, 0.179)
df <- t(data.frame(x,px))
row.names(df) = c("Rockies wins, X","Probability, p(X)")
df %>% kt()
```

What is the probability that the Rockies win zero games? What is the probability that the Rockies win at least two games? Why might the independence assumption be false?
:::

\
\
\
\
\

We may be interested in describing the center or average value of our random variable. We can do this with the following definitions.

:::{.definition}
The ***expected value*** (or ***population mean*** or ***average***) of a random variable $X$ is given by: \

(i) $E[X] = \mu = \sum_{x \in \Omega} x \cdot p(x)$ (for discrete random variables)

(ii) $E[X] = \mu = \int_{x \in \Omega} x \cdot f(x) dx$ (for continuous random variables) 

:::

For this class, evaluating integrals is not essential, so we will avoid using Calculus (integrals and derivatives) when possible.

Sometimes, it makes sense to calculate the expected value of a function of a random variable. This can be easily done with a slight modification to the previous definition. Let $h(X)$ be some function of a random variable $X$. The expected value of $h(X)$, $E[h(X)]$, is given by:

(i) $E[h(X)] = \sum_{x \in \Omega} h(x) \cdot p(x)$ (for discrete random variables)

(ii) $E[h(X)] = \int_{x \in \Omega} h(x) \cdot f(x) dx$ (for continuous random variables) 

:::{.example}
For the Rockies/Cubs four game series example, calculate $E[X]$ and $E[X^2]$.
:::

\
\
\
\
\


The spread or variability associated with a random variable can be calculated using expected values as well.

:::{.definition}
The ***population variance*** of a random variable $X$ is given by: \

(i) $Var(X) = \sum_{x \in \Omega} (x-\mu)^2 \cdot p(x)$ (for discrete random variables)

(ii) $Var(X) = \int_{x \in \Omega} (x-\mu)^2 \cdot f(x) dx$ (for continuous random variables) 

:::


There is also a shortcut formula for calculating variance: \

:::{.theorem}

$Var(X) = E[X^2] - (E[X])^2$

:::

:::{.definition}

The ***population standard deviation*** of a random variable $X$ is given by: \

$SD(X) = \sigma = \sqrt{Var(X)} = \sqrt{E[X^2]-(E[X])^2}$
:::

:::{.example}
For the Rockies/Cubs four game series example, calculate $Var(X)$.
:::

\
\
\
\
\

\newpage

## Common Random Variables

There are several families of random variables that show up frequently in applications. Some of these random variables include:
- Binomial
- Geometric
- Poisson
- Normal

### Binomial RVs

:::{.definition}
A ***binomial(n,p) random variable*** is a discrete random variable that counts the numbers of "successes" over a fixed number of trials, $n$, with each trial having an equal probability of success, $p$.

$P(X=k) = \binom{n}{k} p^k(1-p)^{n-k} = \frac{n!}{k!\ \cdot\ (n-k)!} p^k(1-p)^{n-k}$, where $0 \leq k \leq n, 0 \leq p \leq 1$

If $X \sim Binomial(n,p)$, then $E[X]=np$ and $Var(X)=np(1-p)$
:::

:::{.example}
The Cubs and Rockies are playing a 4-game series. The Rockies have a 0.65 probability of winning each game, and the Cubs have a 0.35 probability. Assume each game is independent. Solve for the following quantities.
:::

(a) The Cubs wins exactly 1 game. \
\
\
\

(b) The Rockies win exactly 2 games.\
\
\
\

(c) The Cubs win at least 2 games.\
\
\
\

(d) The series ends in a sweep. \
\
\
\

(e) The expected number of wins for the Rockies.\
\
\
\

(f) The variance and standard deviations of wins for the Rockies.\
\
\
\

:::{.example}
Complete 10,000 simulations of the four game series between the Rockies and Cubs. For the number of Rockies wins, calculate the sample mean and sample variance and compare these to the population values. Also, plot a histogram of the sample data.
:::

```{r}
set.seed(2020)
rockies_wins <- rbinom(n=10000,size=4,prob=0.65)
mean(rockies_wins)
var(rockies_wins)
rockies_wins_df <- data.frame(Wins=rockies_wins)
rockies_wins_df %>% ggplot(aes(Wins)) + geom_histogram(binwidth = 1,color = "black", fill = "purple")
```

#### Binomial Coefficient Symmetry

Playoff series for a certain sports league are played as a best-of-seven series, with one team hosting four games and the opposing team hosing three. An executive for the league wishes to know the number of ways the home and away games can be assigned. (One such combination is A-A-B-B-A-B-A, the format used by the NBA and NHL for their best-of-seven series.) What is the total number of combinations?\
\
\
\

However, instead of thinking about the number of ways to assign the games to the team that gets four home games, what if we thought about the number of ways to assign games to the team that gets three home games?

That would be $\binom{7}{3}$. We can use the `choose` command in R to find this quantity.

```{r}
choose(7,3)
```

It turns out that this binomial coefficient is also equal to 35.

Theorem: $\binom{n}{k} = \binom{n}{n-k}$

$\binom{n}{k} = \frac{n!}{k!\ \cdot\ (n-k)!}$

$\binom{n}{n-k} = \frac{n!}{(n-k)!\ \cdot\ (n-(n-k))!} = \frac{n!}{(n-k)!\ \cdot\ k!} = \binom{n}{k}$

\newpage

### Geometric RVs

:::{.definition}
A ***Geometric(p) random variable*** is a discrete random variable that counts the numbers of trials until a "success" occurs, where the probability of success, $p$, is constant across all trials.

$P(X=k) = p(1-p)^{k-1}$, where $k \geq 1, 0 \leq p \leq 1$

If $X \sim Geometric(p)$, then $E[X]=\frac{1}{p}$ and $Var(X)=\frac{p}{1-p}$
:::


:::{.example}
Suppose the number of shots needed by a hockey team in order to score their first goal, X, is modeled by a Geometric($\frac{1}{10}$) random variable. Use this information to answer the following questions.
:::

(a) What is the probability that it takes exactly 3 shots to score the first goal? \
\
\
\

(b) What is the probability that it takes less than 3 shots to score the first goal? \
\
\
\

(c) What is the probability that it takes more than 3 shots to score the first goal? \
\
\
\

**Caution:** Some references parameterize the Geometric distribution based on the number of failures before the first success, rather than the trial on which the first success occurs. This changes the PMF, mean, and variance, so be careful.

Let's simulate the number of shot attempts required to score the first goal (Geometric($p=1/10$)) from the previous example.

```{r}
set.seed(2020)
geometric <- rgeom(10000, 1/10)
head(geometric, 20)
```

Some of the values were 0, which could not happen if R was considering the number of the trial on which the first success occurred. You can add 1 to the values given by `R` to arrive at the first success distribution.

```{r}
first_success <- geometric + 1
head(first_success, 20)
mean(first_success)
```
The mean of this sample of variables is 10.827, which is close to the expected mean of $\frac{1}{p} = 10$.

Let's plot the sample distribution of shots required to score a goal from the simulation as well.

```{r}
first_success_df = data.frame(Shots = first_success)
first_success_df %>% ggplot(aes(x=Shots)) + geom_histogram(binwidth = 1)
```

\newpage

### Poisson RVs

:::{.definition}
A ***Poisson($\lambda$) random variable*** is a discrete random variable that counts the numbers of "successes" for a given rate parameter, $\lambda$, for a given interval.

$P(X=k) =  \frac{e^{-\lambda}\lambda^k}{k!}$, where $k \geq 0,$

If $X \sim Poisson(\lambda)$, then $E[X]=\lambda$ and $Var(X)=\lambda$
:::

:::{.example}
During the 2021 Major League Soccer season, the Colorado Rapids scored 51 goals in 34 games on their way to a first-place finish in the Western Conference regular season standings.

The team scored $\frac{51}{34} = 1.5$ goals per game. Let's model the distribution of Rapids goals using a Poisson(1.5) random variable that we'll call Y.
:::

(a) Which is more likely: Y taking on the value 0 or Y taking on the value 2?\
\
\
\

We can calculate these probabilities in R using the `dpois` command.

```{r}
dpois(x=0, lambda=1.5)
dpois(x=2, lambda=1.5)
```

We can also plot the PMF of Y to check visually.

```{r}
ggplot(transform(data.frame(x=c(0:8)), y=dpois(x, lambda = 1.5)), aes(x, y)) + 
  geom_bar(stat="identity") + 
  labs(x="Value", y="Frequency", title="Probability mass function of Poisson(1.5) random variable")
```

Let's check whether using a Poisson distribution was appropriate by comparing it to the actual 2021 Colorado Rapids match results.

```{r}
# Data: https://www.espn.com/soccer/team/results/_/id/184/season/2021

library("kableExtra")

goals <- c(0:4, "5 or more")
actual_frequency <- c(6, 14, 7, 6, 0, 1)
actual_proportion <- actual_frequency / sum(actual_frequency)
expected_proportion <- c(dpois(0:4, lambda=1.5), ppois(4, lambda=1.5, lower.tail=FALSE))
expected_frequency <- round(expected_proportion * 34, 1)

rapids.data <- data.frame(goals, actual_frequency, actual_proportion, expected_frequency, expected_proportion)

rapids.data %>%
kbl() %>%
kable_styling()
```

(b) What differences do you notice between the actual results and the expected values based on the Poisson random variable? \
\
\
\

<!-- There were fewer games in which the Rapids scored 4 or more goals than the model would indicate, yet the Rapids were shut out less often than the model would indicate. -->

(c) Even if the true population distribution of 2021 Rapids goals was truly a Poisson(1.5) random variable, why might the actual distribution of their goals differ from the probability mass function? \
\
\
\

<!-- 34 is a relatively small sample size; random variables may not coincide with their expected values for finite sample sizes. -->

(d) What are the advantages of using the Poisson distribution to model Major League soccer goals? What are the disadvantages? \
\
\
\

<!-- Poisson random variables can take on the natural numbers (including zero), which aligns with the number of goals that can be scored in a match. One disadvantage is that it is possible for a Poisson to take on values that are not realistic for the situation, such as double-digit integers or higher. Only one game in MLS history has had a team score more than seven goals in a game. However, when $\lambda$ is small (such as 1.5), these extreme values are relatively unlikely. -->

:::{.example}
In 1997-1998 with the Los Angeles Lakers, Shaquille O'Neal attempted an average of 11.35 free throws per game with a standard deviation of 4.04. Is it appropriate to model Shaq's per game free throw attempts as a Poisson(11.35) random variable? 
:::

(a) Plot the data.
```{r, message=F}
shaq9798 <- read_csv("data/shaq9798.csv")
shaq9798 %>% ggplot(aes(x=FTA))  + 
  geom_bar(color = "yellow", fill = "purple") +
  ggtitle("Per Game FT Attempt Totals by Shaq in 1997-1998") +
  xlim(0,25)
```
(b) Plot the PMF of a Poisson(11.35) random variable.
```{r}
ggplot(transform(data.frame(x=c(0:25)), y=dpois(x, lambda = 11.35)), aes(x, y)) + 
  geom_bar(stat="identity") + 
  labs(x="Value", y="Frequency", title="Probability mass function of Poisson(11.35) random variable")
```

(c) What similarities and what differences do you notice?\
\
\
\

(d) Calculate the variance of the two distributions and compare them.

```{r}
shaqFTA <- shaq9798 %>% select(FTA)
var(shaqFTA)
# Var(Poisson(11.35)) = 11.35
```

(e) Calculate the probability that Shaq had 20 or more free throws and compare it to $P(Poisson(11.35) \geq 20)$

```{r}
shaq20 <- sum(shaqFTA >= 20)/nrow(shaqFTA); shaq20
poisson20 <- ppois(20, lambda=11.35, lower.tail=FALSE); poisson20
```

(f) Is the Poisson distribution appropriate to model Shaq's FTA per game? Explain. \
\
\
\

\newpage

### Negative Binomial RVs

:::{.definition}
A ***Negative Binomial($r$,$p$) random variable*** is a discrete random variable that counts the numbers of "successes" for given parameters, $r$ and $p$.

$P(X=k) =  {k+r-1 \choose k}(1-p)^rp^k$, where $k \geq 0,$

If $X \sim NB(r,p)$, then $E[X]=\frac{rp}{1-p}$ and $Var(X)=\frac{rp}{(1-p)^2}$
:::

The Negative Binomial distribution is often used to model count data that is "overdispersed". A property of the Poisson distribution is that the mean and variance are equal. If you are analyzing count data such that the variance is much greater than the mean (i.e., overdispersed), then the Negative Binomial distribution may be an appropriate substitute.

Given sample count data, we can estimate appropriate parameters for a Negative Binomial in many ways. One such way is to use the "method of moments" estimator.

These estimators are given by:

$\hat{p} = \frac{s^2-\bar{x}}{s^2}$ and $\hat{r} = \frac{\bar{x}^2}{s^2-\bar{x}}$

:::{.example}
Using Shaq's 1997-1998 data, model his per game free throw attempts as a Negative Binomial random variable.
:::

(a) Find an appropriate choice of parameters, $r$ and $p$.

```{r}
shaq.mean <- mean(shaqFTA$FTA)
shaq.var <- var(shaqFTA$FTA)
rhat <- shaq.mean^2/(shaq.var-shaq.mean)
phat <- (shaq.var-shaq.mean)/shaq.var
c(rhat,phat)
```

(b) Plot the Negative Binomial distribution. Note that R uses an alternative parameterization for $p$. Use $prob = 1-p$.

```{r}
ggplot(transform(data.frame(x=c(0:25)), y=dnbinom(x,size=rhat,prob=1-phat)), aes(x, y)) + 
  geom_bar(stat="identity") + 
  labs(x="Value", y="Frequency", title="Probability mass function of NB(r=25.852,p=0.305) random variable")
```

(c) Calculate the mean and variance of the Negative Binomial and Shaq's dataset.

```{r}
shaq.mean <- mean(shaqFTA$FTA)
shaq.var <- var(shaqFTA$FTA)
NB.mean <- (rhat*phat)/(1-phat)
NB.var <- (rhat*phat)/(1-phat)^2
c(shaq.mean,shaq.var)
c(NB.mean,NB.var)
```

(d) Calculate the probability that Shaq had 20 or more free throws and compare it to $P(NB(r=25.852,p=0.305) \geq 20)$

```{r}
shaq20 <- sum(shaqFTA >= 20)/nrow(shaqFTA); shaq20
nb20 <- pnbinom(20,size=rhat,prob=1-phat,lower.tail=FALSE); nb20
```

(e) Is the Negative Binomial distribution appropriate to model Shaq's FTA per game? How does it compare to using the Poisson distribution? Explain. \
\
\
\

\newpage

### Normal RVs

:::{.definition}
A ***Normal($\mu$,$\sigma^2$) random variable*** is a continuous random variable that is bell-shaped with mean $\mu$ and variance $\sigma^2$.

To calculate probabilities under the normal curve, you need either to integrate, use a table, or a computer.

Note that a normal random variable can be standardized by using: $z = \frac{x-\mu}{\sigma}$
:::

:::{.theorem}
For a normal($\mu$,$\sigma^2$) random variable, we have the following approximations: \
- About 68\% of the data falls within one standard deviation of the mean (i.e., $\mu \pm \sigma$) \
- About 95\% of the data falls within two standard deviations of the mean (i.e., $\mu \pm 2\sigma$) \
- About 99.7\% of the data falls within three standard deviations of the mean (i.e., $\mu \pm 3\sigma$)
:::

:::{.example}
The skills (or tools) of a baseball player are often rated on a scale of 20-80, where 50 is an average grade, 20 is the lowest grade, and 80 is the highest grade. The distribution of tool grades is approximately normally distributed ($\mu=50, \sigma =10$).

See [https://blogs.fangraphs.com/scouting-explained-the-20-80-scouting-scale/](https://blogs.fangraphs.com/scouting-explained-the-20-80-scouting-scale/) for more details. Calculate the following probabilities.
:::

(a) Former Rockie Nolan Arenado has been graded to have game power of 70. Game power estimates a player's ability to hit home runs. Approximately what percentage of baseball players have equal or greater game power than Arenado? \
\
\
\

(b) Mike Trout has been graded to have raw power of 55. Raw power estimates a player's ability to hit baseballs hard (i.e., hard hit rate). Approximately what percentage of baseball players have equal or less raw power than Arenado? \
\
\
\

(c) Suppose a Rockies prospect is said to be in the top 10\% of all baseball players in terms of their speed. What approximate speed grade would correspond to the player? \
\
\
\

(d) Suppose a Rockies prospect is said to be in the bottom 20\% of all baseball players in terms of their hit ability. What approximate hit grade would correspond to the player? \
\
\
\


(e) Between what two grades do approximately 95\% of all players lie for a given tool? \
\
\
\

Let's check our answers:
```{r}
a <- 1-pnorm(q=70,mean=50,sd=10); a
b <- pnorm(q=55,mean=50,sd=10); b
c <- qnorm(0.1,mean=50,sd=10,lower.tail = F); c
d <- qnorm(0.2,mean=50,sd=10,lower.tail = T); d
e <- pnorm(q=70,mean=50,sd=10) - pnorm(q=30,mean=50,sd=10); e
```

\newpage

:::{.example}
Player X has a projected mean WAR of 3 with standard deviation of 2 and player Y has a projected mean WAR of 1.5 with a standard deviation of 3. Assume projected WAR is normally distributed. What is the probability that Player X outperforms Player Y?

Link to WAR explaination: https://www.mlb.com/glossary/advanced-stats/wins-above-replacement
:::

We want Pr(X>Y) or Pr(X-Y>0).  
Let Z = X-Y.  
E[Z]=1.5
Var(Z)=5
Pr(Z>0)=1-Pr(Z $\leq$ 0)

```{r}
#Calculate probability Z<=0
pr <- pnorm(0,1.5,sqrt(5))
print(1-pr)
```

The Probability that Player X outperforms Player Y is 0.7488. 


\newpage


## Extra Stuff




### Sets and Conditional Probability

100 sports fans in Colorado were polled and it was found that 64 had attended either a Denver Nuggets or Colorado Avalanche game at Ball Arena (formerly Pepsi Center). 34 people had seen only a Nuggets game, while 17 had seen both a Nuggets and an Avalanche game.

Q: How many people saw an Avalanche game but not a Nuggets game?

A: 64 - 34 - 17 = 13

Q: What is the probability that a randomly selected person in the poll had been to a Nuggets game?

A: (34 + 17) / 100 = .51

Q: What is the probability that a randomly selected person that had been to a game at Ball Arena had been to a Nuggets game?

A: (34 + 17) / 64 = .797

Q: What is the probability that a randomly selected person had been to a Nuggets game given they had been to an Avalanche game?

A: 17 / (17 + 13) = .567


### Binomials and Multinomials

Suppose we are curious about probabilities regarding the results of a soccer team’s next five games.

Wait!!! A soccer game has three possible outcomes (win, lose, draw)! We can’t use the binomial distribution, since it limits us to two possible outcomes!

It depends. If we are interested in the probability that a soccer team wins 2 of their next 5 games, we can use the binomial distribution. We can create the following partition of the sample space of outcomes: $(Win)$ and $(Win^C)$, where the second set includes both losing and drawing.

Then, the formula would be represented as:

$\binom{5}{2}\ P(Win)^2\  P(Win^C)^{(5-2)}$

If we are interested in the probability of the team winning two of the next five games, drawing two, and losing one, we cannot use the binomial theorem. That involves three outcomes, and would be represented as a multinomial. <!--- assuming regularity conditions such as independence. -->



### Expectation - Baseball

The expectation of a discrete random variable is a weighted average. The "weights" are the probabilities of the possible values of the variable.

Consider the following table, which shows the number of career hits by type for the all-time Major League Baseball leader in total bases, Hank Aaron.

<!-- https://www.baseball-reference.com/players/a/aaronha01.shtml -->

```{r expectation baseball, echo=F}
Hit_type <- c("Single", "Double", "Triple", "Home Run")
Number_bases <- c(1:4)
Hit_Frequency <- c(2294, 624, 98, 755)
Hit_Proportion <- round(Hit_Frequency / sum(Hit_Frequency), 8)

Hank_Aaron_Hits <- data.frame(Hit_type, Number_bases, Hit_Frequency, Hit_Proportion)

Hank_Aaron_Hits %>%
kbl() %>%
kable_styling()
```

The expected number of bases for a Hank Aaron hit is the sum of the number of bases attained for each hit multiplied by the relative frequency of the occurrence of that type of hit.

$1 \cdot \frac{2294}{3771} + 2 \cdot \frac{624}{3771} + 3 \cdot \frac{98}{3771} + 4 \cdot \frac{755}{3771} = 1.18181$

This is the same process that is occurring whenever we calculate the expectation of any discrete random variable. Recall the formula for expectation is $E[X] = \sum_{x \in \Omega}\ x \cdot p(x)$. Each value in the sample space is "adjusted" by the probability of that value, then the sum of all values in $\Omega$ is taken to arrive at the weighted average, or expected value, of the random variable.

### Basketball Scenario

You are the coach of a basketball team that is down two points with one second remaining in the fourth quarter. During a timeout, you are considering the best play to call for your team. The first option is a three-point shot attempt, which you estimate has a 30% chance of succeeding. The second option is a two-point shot attempt, which has a 50% chance of making the field goal, a 30% chance of missing it and ending the game, and a 20% chance the shooter will miss but be fouled, in which case the shooter's free throw success will follow a $Bin(2, .8)$ random variable. Finally, you estimate that your team's probability of winning the game in overtime is .45.

Assume the above situations are exhaustive (i.e., the other team will not get another possession, no fouls will be called before the ball is put in play, lightning will not hit the arena and postpone the game, etc.). Which of the two plays should you call to maximize the win probability for your team?

A: The probability of winning the game with the three-point shot attempt is .3. If the two-point shot attempt is called for, there is a .5 probability of making the field goal and a (.2)(.8)(.8) = .128 probability that the foul is called and both free throws are made. Thus, the total probability of scoring two points and sending the game to overtime is .628. Then, the probability of winning the game in OT after tying it in regulation is (.628)(.45) = .2828. This is less than .3, so shooting the three-pointer is the option that maximizes the win probability, given these situational probabilities.

Q: What is the minimum estimated overtime win probability to make calling for the two-point play the better option?

A: $P(score\ 2\ points\ in\ regulation) \cdot P(win\ in\ OT) > P(win\ in\ regulation)$\
$.628 \cdot P(win\ in\ OT) > .3$\
$P(win\ in\ OT) > .478$

### Multiple Probability Distributions - Basketball

Suppose the number of points scored by a basketball player follows a Poisson(12) random variable, the number of rebounds by a Poisson(7) distribution, and assists by a Discrete Uniform(2, 11), independently of each other.

Q: What is the probability that this player records a points, rebounds, assists triple-double in a game?

A: $P(Triple\ Double) = P(Points \geq 10\ \cap\ Rebounds \geq 10\ \cap\ Assists \geq 10)$

```{r basketball probability 1}
ppois(9, lambda = 12, lower.tail=F)
```

$P(Points \geq 10) = P(Poisson(12) \geq 10) \approx .758$

```{r basketball probability 2}
ppois(9, lambda = 7, lower.tail=F)
```

$P(Rebounds \geq 10) = P(Poisson(7) \geq 10) \approx .170$

$P(Assists \geq 10) = P(Discrete\ Uniform(2, 11) \geq 10) = .2$

Since the events are independent, we can multiply their probabilities. The probability of the player scoring the triple-double is $(.758)(.170)(.2) = .0257$.

Q: Your friend offers you 4 to 1 that the player will not record a triple-double in their next 10 games. With the knowledge that the athlete's performance in a game is unaffected by performances in previous games, would you take the bet?

A: $P(no\ triple\ double) = 1 - .0257 = .9743$, so $P(no\ triple\ double\ in\ next\ 10\ games) = (.9743)^{10} = .771$

The odds of no triple-double are $\frac{.771}{1-.771} = 3.37$, so the bet of no triple-double at 4 to 1 odds is favorable.



*answers may vary for following questions*

Q: What differences do you notice between the actual results and the expected values based on the Poisson random variable?

A: There were fewer games in which the Rapids scored 4 or more goals than the model would indicate, yet the Rapids were shut out less often than the model would indicate.

Q: Even if the true population distribution of 2021 Rapids goals was truly a Poisson(1.5) random variable, why might the actual distribution of their goals differ from the probability mass function?

A: 34 is a relatively small sample size; random variables may not coincide with their expected values for finite sample sizes.

Q: What are the advantages of using the Poisson distribution to model Major League soccer goals? What are the disadvantages?

A: Poisson random variables can take on the natural numbers (including zero), which aligns with the number of goals that can be scored in a match. One disadvantage is that it is possible for a Poisson to take on values that are not realistic for the situation, such as double-digit integers or higher. Only one game in MLS history has had a team score more than seven goals in a game. However, when $\lambda$ is small (such as 1.5), these extreme values are relatively unlikely.


### Law of Total Probability - Baseball

You work in the front office of a professional baseball club and have just learned that a certain prospect hits .200 against left-handed pitchers and .400 against right-handed pitchers (their overall batting average is unknown). The general manager of the team overhears you talking about the .400 statistic of the player and becomes very exited that they have the chance to draft a .400 hitter. What would you say to caution the GM that the player might not be a remarkable hitter?

A: We don't know the proportion of the player's at-bats that came against left-handed pitchers versus right-handed pitchers. If we want to know the player's batting average unconditional on the type of pitcher they are facing, we have to adjust $P(hit\ |\ left-handed\ pitcher)$ by $P(left-handed\ pitcher)$ and $P(hit\ |\ right-handed\ pitcher)$ by $P(right-handed\ pitcher)$ before adding them to determine $P(hit)$. For example, if 90% of the player's at-bats were against left-handed pitchers, then their overall batting average is a pedestrian .220.

 
*Other possible issues: low sample size of player's at-bats, the fact that pro pitchers will be harder to hit against than non-pros*

### Multinomial Distribution - Baseball

The following PMF displays the number of possible bases for a hit in baseball against the frequency of that type of hit for Hank Aaron, who is third all-time in Major League baseball hits with 3,771.

```{r multinomial, include=F}
Number_bases <- c(1:4)
Hit_Frequency <- c(2294, 624, 98, 755)
Hit_Proportion <- round(Hit_Frequency / sum(Hit_Frequency), 5)

Hank_Aaron_PMF <- data.frame(Number_bases, Hit_Proportion); Hank_Aaron_PMF
```

| Number of Bases, $x$  | 1 | 2 | 3 | 4 |
|:---------------------:|:-:|:-:|:-:|:-:|
| Probability, $P(X=x)$ | 0.61 | 0.16 | 0.03 | 0.20 |

Q: What is the probability that a randomly selected subset of 5 hits from Hank Aaron includes three singles, one double, and one home run?

<!-- note: assumes sampling with replacement. Without replacement would be a multivariate hypergeometric and would slightly increase the probability.  -->

A: From the Multinomial PMF, this is $P(X_1 = 3, X_2 = 1, X_3 = 0, X_4 = 1) = \frac{5!}{3!\ \cdot 1!\ \cdot 0!\ \cdot 1!}\ (.61)^3\cdot\ (.16)^1\cdot\ (.03)^0\cdot\ (.20)^1 \approx .145$

The probability can also be found in `R` using the `dmultinom` function.

```{r multinomial 2}
dmultinom(x = c(3, 1, 0, 1), prob = c(.61, .16, .03, .20))
```


### Bayes - injured baseball player

:::{.example}
A runner on first base with 2 out and nobody else on base will attempt to steal second base on the first pitch 70% of the time if he is fully healthy but only 10% of the time if he is playing through an injury. Assume that 80% of the player population is healthy. You see a randomly selected runner not attempt a steal in this situation. What is the probability that the runner is playing through an injury?
:::

From Bayes Theorem:  

Pr(Injury given No Steal) = Pr(No Steal given Injury)\*Pr(Injury)/P(No Steal).  

Pr(No Steal given Injury) = 1 - Pr(Steal given Injury) = 0.9.  

Pr(Injury) = 1- Pr(Healthy) = 0.2.  

Pr(No Steal) = Pr(No Steal given Injury)\*Pr(Injury)+Pr(No Steal given Healthy)\*Pr(Healthy).  

Pr(No Steal) = 0.9\*0.2+0.7\*0.8 = 0.74.  

Therefore Pr(Injury given No Steal) = 0.9\*0.2/0.74 = 0.243.  
