---
output:
  pdf_document: default
  html_document: default
---
```{r, echo=F, message=F,warning=F}
library(tidyverse)
library(kableExtra)
library(knitr)
knitr::opts_chunk$set(tidy=FALSE,message=F,warning=F)

# kable table global setup
kt <- function(data) {
  knitr::kable(data, digits=3, align=c('l','c','c','c','c','c','c','c','c')) %>% kable_styling(bootstrap_options='striped', latex_options='HOLD_position', full_width = F, position = "center")
}
```


# Advanced Regression Methods

## Weighted Least Squares Regression

Recall from Example 8.1 where we attempted to predict ERA in 2022 from ERA and FIP from 2021. One concern in this analysis was that each pitcher pitched a different number of innings. We'd like to account for this by giving more weight to pitchers with more innings pitched. This can be accomplished by using weighted least squares regression. Weighted least squares regression is particularly used when the data exhibits non-constant variance

:::{.example}
Build SLR models predicting ERA22 from ERA21 with and without weights.
:::

```{r,warning=F}
# Download individual pitching data for 2021 and 2022 seasons
library(tidyverse)
library(broom)
library(baseballr)

pit21 <- bref_daily_pitcher("2021-01-01", "2021-12-31") %>% 
  fip_plus() %>% 
  dplyr::select(Name, IP, ERA, FIP) %>%
  dplyr::arrange(dplyr::desc(IP)) %>%
  mutate(IP21=IP,ERA21=ERA,FIP21=FIP)

pit22 <- bref_daily_pitcher("2022-01-01", "2022-12-31") %>% 
  fip_plus() %>% 
  dplyr::select(Name, IP, ERA, FIP) %>%
  dplyr::arrange(dplyr::desc(IP)) %>%
  mutate(IP22=IP,ERA22=ERA,FIP22=FIP)

# merge the datasets together, remove redundant columns
all_pit <- pit21 %>% 
  left_join(pit22,by = "Name") %>% 
  select(-c(2:4,8:10)) %>%
  filter(IP21>5 & IP22 > 5)

# merge the datasets together, remove redundant columns
all_pit_min0 <- pit21 %>% left_join(pit22,by = "Name") %>% 
  select(-c(2:4,8:10)) %>% filter(IP21>0 & IP22>0)

all_pit_min5 <- pit21 %>%  left_join(pit22,by = "Name") %>% 
  select(-c(2:4,8:10)) %>% filter(IP21>5 & IP22>5)
```

```{r,fig.height=3}
all_pit %>% ggplot(aes(x=ERA21,y=ERA22,size=IP21+IP22)) + geom_point(alpha=0.2,color="blue") +
  scale_size(range = c(0,5))
```

\newpage

```{r}
mod_slr <- lm(ERA22~ERA21,data=all_pit_min0)
mod_wls <- lm(ERA22~ERA21,data=all_pit_min0,weights = IP21)
```

```{r,echo=F}
mod_slr %>% tidy() %>%
  mutate(
    p.value = scales::pvalue(p.value),
    term = c("Intercept","Earned Run Average, 2021")
  ) %>%
  kable(booktabs=T,digits=c(3,3,3,3), 
        caption = "SLR Model Estimating ERA2022 using ERA2021 (equal weights)",
        col.names = c("Predictor", "Estimate", "Std Error", "t stat", "p-value")) %>%
  kable_styling(latex_options = "hold_position")
```

```{r,echo=F}
mod_wls %>% tidy() %>%
  mutate(
    p.value = scales::pvalue(p.value),
    term = c("Intercept","Earned Run Average, 2021")
  ) %>%
  kable(booktabs=T,digits=c(3,3,3,3), 
        caption = "SLR Model Estimating ERA2022 using ERA2021 (weighted by IP)",
        col.names = c("Predictor", "Estimate", "Std Error", "t stat", "p-value")) %>%
  kable_styling(latex_options = "hold_position")
```


```{r}
mod_slr <- lm(ERA22~ERA21,data=all_pit_min5)
mod_wls <- lm(ERA22~ERA21,data=all_pit_min5,weights = IP21)
```

```{r,echo=F}
mod_slr %>% tidy() %>%
  mutate(
    p.value = scales::pvalue(p.value),
    term = c("Intercept","Earned Run Average, 2021")
  ) %>%
  kable(booktabs=T,digits=c(3,3,3,3), 
        caption = "SLR Model Estimating ERA2022 using ERA2021 (equal weights)",
        col.names = c("Predictor", "Estimate", "Std Error", "t stat", "p-value")) %>%
  kable_styling(latex_options = "hold_position")
```

```{r,echo=F}
mod_wls %>% tidy() %>%
  mutate(
    p.value = scales::pvalue(p.value),
    term = c("Intercept","Earned Run Average, 2021")
  ) %>%
  kable(booktabs=T,digits=c(3,3,3,3), 
        caption = "SLR Model Estimating ERA2022 using ERA2021 (weighted by IP)",
        col.names = c("Predictor", "Estimate", "Std Error", "t stat", "p-value")) %>%
  kable_styling(latex_options = "hold_position")
```




\newpage

## Stepwise Regression using Cross-Validation

Cross-validation is a resampling method that uses different portions of the data to train and test models. It is often used when we aim to do prediction.

:::{.example}
Using the 2022 MLB team statistics data, use cross-validation to determine a linear model with runs/game as the response and BA, OBP, SLG, and OPS as predictors. Use 10-fold cross-validation with 3 repeats.
:::

```{r}
# Acquire the data
library(rvest)
url <- "https://www.baseball-reference.com/leagues/majors/2022.shtml"
site <- read_html(url)
mlb22 <- site %>% html_elements("#teams_standard_batting") %>% html_table()
mlb22 <- mlb22 %>% data.frame() %>% column_to_rownames("Tm") %>%
  rename(`R/G`=R.G) %>% slice(-(31:33)) %>% select(`R/G`,BA,OBP,SLG,OPS)
mlb22 <- write_csv(mlb22,"data/mlb22.csv")
```


```{r,message=F}
mlb22 <- read_csv("data/mlb22.csv")
set.seed(2023)
library(caret)
mod_null <- lm(`R/G` ~ 1, data=mlb22)
mod_full <- lm(`R/G` ~ ., data=mlb22)
set_train <- trainControl(method="repeatedcv", number=10, repeats=3)
mod_cv <- train(`R/G` ~ ., data=mlb22,  scope = formula(mod_null),
  method="lmStepAIC", direction="both", trace=FALSE, trControl=set_train)
```

```{r}
# Null model
coef(mod_null)
```

```{r}
# Full model with all variables
coef(mod_full)
```

```{r}
# Model selected using stepwise selection with CV
coef(mod_cv$finalModel)
```

\newpage

## Ridge Regression

Ridge regression is a method for estimating coefficients of a multiple regression model and used particularly used when predictor variables are highly correlated.

In particular, model estimates of $\hat{\beta}_i$ are calculated to minimize:

$\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \beta_j^2$

:::{.example}
NHL team statistics for the 2021-2022 season is contained in `nhl21-22.csv`. The goal is to build a ridge regression model with team points as the response and other team statistics as the predictors.
:::

(a) Load the data and display in a kable table.

```{r,warning=F,message=F}
# Load data and output a subset of the table
nhl2122 <- read_csv("data/nhl21-22.csv")
nhl2122$Team <- gsub("\\*", "",nhl2122$Team)
nhl2122 %>% select(1:10) %>% slice(1:10) %>% kable(booktabs=T)
```

\newpage

(b) Fit the optimal value for lambda.

```{r}
library(glmnet)
set.seed(2023)

y <- nhl2122$PTS
x <- nhl2122 %>% select(-Team,-PTS) %>% scale() %>% data.matrix()
lambdas <- 10^seq(3, -3, by = -.1)

# fit the ridge regression model
model <- glmnet(x, y, alpha = 0, lambda = lambdas)

# fit the ridge regression model using cross-validation
cv_model <- cv.glmnet(x, y, alpha = 0, lambda = lambdas)
( best_lambda <- cv_model$lambda.min )
```


(c) Plot MSE as a function of log(lambda).

```{r}
plot(cv_model)
```

\newpage

(d) Create a trace plot.

```{r}
# Trace plot
# Note that coefficients shrink to zero as lambda gets large
# When lambda is small, we get ordinary least squares regression
plot(model,xvar="lambda",label = T) 
```

(e) Find the final ridge regression model.

```{r}
# Use optimal lambda to find final ridge regression model
best_model <- glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(best_model)
```

\newpage

## Lasso Regression

Lasso regression is a method for estimating coefficients of a multiple regression model and is especially useful for variable selection.

In particular, model estimates of $\hat{\beta}_i$ are calculated to minimize:

$\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p |\beta_j|$

:::{.example}
Use the NHL 2021-2022 team statistics dataset to fit a regression model using LASSO.
:::

(a) Fit the optimal value for lambda.

```{r}
set.seed(2023)

y <- nhl2122$PTS
x <- nhl2122 %>% select(-Team,-PTS) %>% scale() %>% data.matrix()
lambdas <- 10^seq(2, -2, by = -.1)

# fit the ridge regression model
model <- glmnet(x, y, alpha = 1, lambda = lambdas)

# fit the ridge regression model using cross-validation
cv_model <- cv.glmnet(x, y, alpha = 1, lambda = lambdas)
( best_lambda <- cv_model$lambda.min )
```


(b) Plot MSE as a function of log(lambda).

```{r,fig.height=4}
plot(cv_model)
```

\newpage

(c) Create a trace plot. 

```{r,message=F,warning}
# Trace plot
# Note that coefficients shrink to zero as lambda gets large
# When lambda is small, we get ordinary least squares regression
plot(model,xvar="lambda",label = T) 
```

(d) Find the final lasso regression model.

```{r}
# Use optimal lambda to find final lasso regression model
lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(lasso_model)
```

\newpage

## Elastic Net

Elastic Net Regularization is a method for estimating coefficients of a multiple regression model by combining Ridge and LASSO Regression

In particular, model estimates of $\hat{\beta}_i$ are calculated to minimize:

$\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \cdot \left( \alpha \cdot \sum_{j=1}^p |\beta_j| + \frac{1-\alpha}{2} \sum_{j=1}^p \beta_j^2 \right)$

:::{.example}
Use the NHL 2021-2022 team statistics dataset to fit a regression model using Elastic Net and cross-validation.
:::

(a) First tune the model parameters, $\alpha$ and $\lambda$.

```{r}
set.seed(2023)
library(caret)
cv5 = trainControl(method = "cv", number = 5)
elnet = train(PTS~.-Team,data=nhl2122,
  metric = "RMSE",
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(
    .alpha = seq(0, 1, length.out = 10), 
    .lambda = seq(0, 5, length.out = 101)),
  method = "glmnet", trControl = cv5)
elnet$bestTune
```

\newpage

(b) Find the final model after tuning.
 
```{r}
elastic_mod <- glmnet(x, y, alpha = elnet$bestTune$alpha, lambda = elnet$bestTune$lambda)
coef(elastic_mod)
```
\newpage

## Mixed Effects Models

Mixed effects regression models allow us to model model predictor variables as fixed or random factors.

\textbf{Fixed Factors vs Random Factors}

The term factor refers to a variable that is thought to influence the outcome of an experiment.
A factor has levels, which refer the the specific values of that factor. For example, team
might be a factor with 32 levels, one for each team.


(i) \textbf{Fixed Factors:} the specific factors that were used in the experiment, and their levels,
are of direct interest to the researcher. Conclusions of the analysis apply to the factors
and the specific factor levels tested. Example: Are the basketball \textbf{four factors} predictive of team winning percentage?

(ii) \textbf{Random Factor:} the levels of a random factor represent a random sample from a
larger population of possible levels. Example: How much variability in win percentage is explained
by team? Say there are 32 possible teams of data (levels), and we would like to account for the variability associated with team, however, we aren't interested in estimating the effect of team, just the variability associated with team.


\textbf{Fixed Effect vs Random Effect}

The term effect refers to a variable's coefficient, or in some cases, the difference between two
coefficients.

(i) \textbf{Fixed Effects:} effects or coefficients associated with fixed factors (these effects are of
direct interest)

(ii) \textbf{Random Effects:} effects or coefficients associated with random factors (these effects are
NOT of direct interest; the variation among effects is of interest)

\textbf{Some Mixed Effects Models:} \

\vfill

\textbf{More about Basketball Four Factors}

For more information about the Four Factors, see:

[https://www.basketball-reference.com/about/factors.html](https://www.basketball-reference.com/about/factors.html)

The following video playlist also discusses advanced basketball analytics:

[https://www.youtube.com/playlist?list=PLtzZl14BrKjTJZdubjNEY5jU0fGOiy51x](https://www.youtube.com/playlist?list=PLtzZl14BrKjTJZdubjNEY5jU0fGOiy51x)



\newpage

:::{.example}
In this exercise, we will assess the relationship between the Four Factors and Win Percentage in professional basketball using team statistics from 2000-2022.
:::

(a) Scrape advanced team statistics from Basketball Reference for the years 2000-2022. We are interested in the following variables: Team, Four Factors, Season.

```{r, cache=T}
url_base <- "https://www.basketball-reference.com/leagues/NBA_"
year = 2000:2022
n_year = length(year)
url_end <- ".html"
library(rvest)
nba_team_data <- NULL

for(i in 1:n_year){
  url <- paste(url_base,year[i],url_end,sep = "")
  site <- read_html(url)
  temp_table <- site %>% html_element("#advanced-team") %>% html_table() %>% data.frame()
  names(temp_table) <- as.character(temp_table[1,])
  temp_table <- temp_table[-1,] %>% 
    select(1:22) %>% 
    filter(Team != "League Average") %>% 
    mutate(Team = gsub("\\*", "", Team))
  teams <- temp_table %>% select(Team) %>% slice(1:30)
  temp_table[,3:22] <- temp_table %>% select(3:22) %>%
    mutate_if(is.character, as.numeric)
  temp_table <- temp_table %>%
    mutate(WinPct = W/(W+L)) %>%
    select(Team,`WinPct`,`eFG%`,`TOV%`,`ORB%`,`FT/FGA`) %>% 
    mutate(Season=year[i])
  nba_team_data <- rbind(nba_team_data,temp_table)
}
nba_team_data <- nba_team_data %>% as.data.frame()
```

```{r}
nba_team_data %>% slice(1:5) %>% kable(booktabs=T,digits=3)
```

\newpage

(b) Inspect the correlational structure of the dataset

```{r,message=F,warning=F}
nba_team_data %>% select(-Team,-Season) %>% ggpairs()
```


\newpage

(c) Fit a mixed effects model with team modeled as a random effect and the four factors as fixed effects.

```{r}
library(lme4)
nba_team_data_scaled <- nba_team_data %>% mutate_if(is.numeric,scale)
nba_model <- lmer(WinPct~(1|Team)+`eFG%`+`TOV%`+`ORB%`+`FT/FGA`,
                  data=nba_team_data_scaled)
summary(nba_model)
```

\newpage

(d) Fit a mixed effects model with team and season modeled as random effects and the four factors as fixed effects.

```{r}
library(lme4)
nba_team_data_scaled <- nba_team_data %>% mutate_if(is.numeric,scale)
nba_model <- lmer(WinPct~(1|Team)+(1|Season)+`eFG%`+`TOV%`+`ORB%`+`FT/FGA`,
                  data=nba_team_data_scaled)
summary(nba_model)
```


